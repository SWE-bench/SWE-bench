{"repo": "scikit-learn/scikit-learn", "pull_number": 30521, "instance_id": "scikit-learn__scikit-learn-30521", "issue_numbers": ["30131"], "base_commit": "eefcb113a410cc7cb15b16ebbbec3156832f8fdb", "patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/30521.fix.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/30521.fix.rst\nnew file mode 100644\nindex 0000000000000..537c3760b16df\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/30521.fix.rst\n@@ -0,0 +1,4 @@\n+-  |Enhancement| Added a new paramenter `tol` to\n+   :class:`linear_model.LinearRegression` that determines the precision of the\n+   solution `coef_` when fitting on sparse data. :pr:`30521` by :user:`Success Moses\n+   <SuccessMoses>`.\ndiff --git a/sklearn/linear_model/_base.py b/sklearn/linear_model/_base.py\nindex bb71cbe9ed550..6aa2e3e6563dc 100644\n--- a/sklearn/linear_model/_base.py\n+++ b/sklearn/linear_model/_base.py\n@@ -8,7 +8,7 @@\n import numbers\n import warnings\n from abc import ABCMeta, abstractmethod\n-from numbers import Integral\n+from numbers import Integral, Real\n \n import numpy as np\n import scipy.sparse as sp\n@@ -32,6 +32,7 @@\n     indexing_dtype,\n     supported_float_dtypes,\n )\n+from ..utils._param_validation import Interval\n from ..utils._seq_dataset import (\n     ArrayDataset32,\n     ArrayDataset64,\n@@ -472,6 +473,15 @@ class LinearRegression(MultiOutputMixin, RegressorMixin, LinearModel):\n     copy_X : bool, default=True\n         If True, X will be copied; else, it may be overwritten.\n \n+    tol : float, default=1e-4\n+        The precision of the solution (`coef_`) is determined by `tol` which\n+        specifies a different convergence criterion for the `lsqr` solver.\n+        `tol` is set as `atol` and `btol` of `scipy.sparse.linalg.lsqr` when\n+        fitting on sparse training data. This parameter has no effect when fitting\n+        on dense data.\n+\n+        .. versionadded:: 1.7\n+\n     n_jobs : int, default=None\n         The number of jobs to use for the computation. This will only provide\n         speedup in case of sufficiently large problems, that is if firstly\n@@ -555,6 +565,7 @@ class LinearRegression(MultiOutputMixin, RegressorMixin, LinearModel):\n         \"copy_X\": [\"boolean\"],\n         \"n_jobs\": [None, Integral],\n         \"positive\": [\"boolean\"],\n+        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n     }\n \n     def __init__(\n@@ -562,11 +573,13 @@ def __init__(\n         *,\n         fit_intercept=True,\n         copy_X=True,\n+        tol=1e-4,\n         n_jobs=None,\n         positive=False,\n     ):\n         self.fit_intercept = fit_intercept\n         self.copy_X = copy_X\n+        self.tol = tol\n         self.n_jobs = n_jobs\n         self.positive = positive\n \n@@ -668,11 +681,13 @@ def rmatvec(b):\n             )\n \n             if y.ndim < 2:\n-                self.coef_ = lsqr(X_centered, y)[0]\n+                self.coef_ = lsqr(X_centered, y, atol=self.tol, btol=self.tol)[0]\n             else:\n                 # sparse_lstsq cannot handle y with shape (M, K)\n                 outs = Parallel(n_jobs=n_jobs_)(\n-                    delayed(lsqr)(X_centered, y[:, j].ravel())\n+                    delayed(lsqr)(\n+                        X_centered, y[:, j].ravel(), atol=self.tol, btol=self.tol\n+                    )\n                     for j in range(y.shape[1])\n                 )\n                 self.coef_ = np.vstack([out[0] for out in outs])\n", "test_patch": "diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py\nindex d7a201f3abf6f..98f3ab21b9e16 100644\n--- a/sklearn/tests/test_pipeline.py\n+++ b/sklearn/tests/test_pipeline.py\n@@ -371,7 +371,7 @@ def test_pipeline_raise_set_params_error():\n     # expected error message for invalid inner parameter\n     error_msg = re.escape(\n         \"Invalid parameter 'invalid_param' for estimator LinearRegression(). Valid\"\n-        \" parameters are: ['copy_X', 'fit_intercept', 'n_jobs', 'positive'].\"\n+        \" parameters are: ['copy_X', 'fit_intercept', 'n_jobs', 'positive', 'tol'].\"\n     )\n     with pytest.raises(ValueError, match=error_msg):\n         pipe.set_params(cls__invalid_param=\"nope\")\ndiff --git a/sklearn/utils/_test_common/instance_generator.py b/sklearn/utils/_test_common/instance_generator.py\nindex bac401d8d657f..c46213b417090 100644\n--- a/sklearn/utils/_test_common/instance_generator.py\n+++ b/sklearn/utils/_test_common/instance_generator.py\n@@ -575,6 +575,7 @@\n             dict(positive=False),\n             dict(positive=True),\n         ],\n+        \"check_sample_weight_equivalence_on_sparse_data\": [dict(tol=1e-12)],\n     },\n     LocallyLinearEmbedding: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1)},\n     LogisticRegression: {\n@@ -983,18 +984,6 @@ def _yield_instances_for_check(check, estimator_orig):\n     KNeighborsTransformer: {\n         \"check_methods_sample_order_invariance\": \"check is not applicable.\"\n     },\n-    LinearRegression: {\n-        # TODO: this model should converge to the minimum norm solution of the\n-        # least squares problem and as result be numerically stable enough when\n-        # running the equivalence check even if n_features > n_samples. Maybe\n-        # this is is not the case and a different choice of solver could fix\n-        # this problem. This might require setting a low enough value for the\n-        # tolerance of the lsqr solver:\n-        # https://github.com/scikit-learn/scikit-learn/issues/30131\n-        \"check_sample_weight_equivalence_on_sparse_data\": (\n-            \"sample_weight is not equivalent to removing/repeating samples.\"\n-        ),\n-    },\n     LinearSVC: {\n         # TODO: replace by a statistical test when _dual=True, see meta-issue #16298\n         \"check_sample_weight_equivalence_on_dense_data\": (\n", "problem_statement": "LinearRegression on sparse matrices is not sample weight consistent\nPart of #16298.\r\n\r\n### Describe the bug\r\n\r\nWhen using a sparse container like `csr_array` for `X`, `LinearRegression` even fails to give the same coefficients for unit or no sample weight, and more generally fails the `test_linear_regression_sample_weight_consitency` checks. In that setting, the underlying solver is `scipy.sparse.linalg.lsqr`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.utils.fixes import csr_array\r\nfrom sklearn.datasets import make_regression\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.utils._testing import assert_allclose\r\n\r\nX, y = make_regression(100, 100, random_state=42)\r\nX = csr_array(X)\r\nreg = LinearRegression(fit_intercept=True)\r\nreg.fit(X, y)\r\ncoef1 = reg.coef_\r\nreg.fit(X, y, sample_weight=np.ones_like(y))\r\ncoef2 = reg.coef_\r\nassert_allclose(coef1, coef2, rtol=1e-7, atol=1e-9)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe `assert_allclose` should pass.\r\n\r\n### Actual Results\r\n\r\n```Python Traceback\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=1e-09\r\n\r\nMismatched elements: 100 / 100 (100%)\r\nMax absolute difference among violations: 0.00165048\r\nMax relative difference among violations: 0.02621317\r\n ACTUAL: array([-2.450778e-01,  2.917985e+01,  1.678916e+00,  7.534454e+01,\r\n        1.241587e+01,  1.076716e+00, -4.975206e-01, -9.262295e-01,\r\n       -1.373931e+00, -1.624112e-01, -8.644422e-01, -5.986218e-01,...\r\n DESIRED: array([-2.452359e-01,  2.918078e+01,  1.678681e+00,  7.534410e+01,\r\n        1.241459e+01,  1.076624e+00, -4.962305e-01, -9.257701e-01,\r\n       -1.373862e+00, -1.622824e-01, -8.652183e-01, -5.981715e-01,...\r\n```\r\n\r\nThe test also fails for `fit_intercept=False`. Note that this test and other sample weight consistency checks pass if we do not wrap `X` in a sparse container.\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:32:50) [Clang 16.0.6 ]\r\nexecutable: /Users/abaker/miniforge3/envs/sklearn-dev/bin/python\r\n   machine: macOS-14.5-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.dev0\r\n          pip: 24.2\r\n   setuptools: 73.0.1\r\n        numpy: 2.1.0\r\n        scipy: 1.14.1\r\n       Cython: 3.0.11\r\n       pandas: 2.2.2\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libopenblas.0.dylib\r\n        version: 0.3.27\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libomp\r\n       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libomp.dylib\r\n        version: None\r\n```\r\n\r\nEDIT: discovered while working on #30040 for the case of dense inputs.\n", "hints_text": "Note, that this is not the case for `Ridge(solver=\"lsqr\", alpha=0, tol=1e-12)`:\r\n\r\n```python\r\n>>> from sklearn.utils.fixes import csr_array\r\n... from sklearn.datasets import make_regression\r\n... from sklearn.linear_model import Ridge\r\n... from sklearn.utils._testing import assert_allclose\r\n... \r\n... X, y = make_regression(100, 100, random_state=42)\r\n... X = csr_array(X)\r\n... reg = Ridge(solver=\"lsqr\", alpha=0, fit_intercept=True, tol=1e-12)\r\n... reg.fit(X, y)\r\n... coef1 = reg.coef_\r\n... reg.fit(X, y, sample_weight=np.ones_like(y))\r\n... coef2 = reg.coef_\r\n... assert_allclose(coef1, coef2, rtol=1e-7, atol=1e-9)\r\n```\r\n\r\nBut the same fails with a larger `tol` value, which is passed as `atol` & `btol` parameters to the `lsqr` call:\r\n\r\n```python\r\n>>> reg = Ridge(solver=\"lsqr\", alpha=0, fit_intercept=True, tol=1e-4)\r\n... reg.fit(X, y)\r\n... coef1 = reg.coef_\r\n... reg.fit(X, y, sample_weight=np.ones_like(y))\r\n... coef2 = reg.coef_\r\n... assert_allclose(coef1, coef2, rtol=1e-7, atol=1e-9)\r\nTraceback (most recent call last):\r\n  Cell In[9], line 13\r\n    assert_allclose(coef1, coef2, rtol=1e-7, atol=1e-9)\r\n  File ~/code/scikit-learn/sklearn/utils/_testing.py:232 in assert_allclose\r\n    np_assert_allclose(\r\n  File ~/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py:1688 in assert_allclose\r\n    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\r\n  File ~/miniforge3/envs/dev/lib/python3.12/contextlib.py:81 in inner\r\n    return func(*args, **kwds)\r\n  File ~/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py:889 in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=1e-09\r\n\r\nMismatched elements: 100 / 100 (100%)\r\nMax absolute difference among violations: 0.00143505\r\nMax relative difference among violations: 0.03345462\r\n ACTUAL: array([ 5.135931e-01,  3.037594e+01,  2.145667e+00,  7.394602e+01,\r\n        1.324545e+01, -4.438229e-02, -1.202226e+00,  7.553014e-01,\r\n       -1.661572e+00,  1.483041e-01, -3.624484e-01, -1.915631e+00,...\r\n DESIRED: array([ 5.125267e-01,  3.037609e+01,  2.145875e+00,  7.394601e+01,\r\n        1.324581e+01, -4.334871e-02, -1.202492e+00,  7.555202e-01,\r\n       -1.661108e+00,  1.495079e-01, -3.613935e-01, -1.916570e+00,...\r\n```\r\n\r\nI think we need to pass `atol=self.tol` / `btol=self.tol` to the inner call to `lsqr` and expose a tolerance parameter as we do in ridge. Or alternatively set a default strict, `dtype`-dependent value as we now do for the `cond` parameter in #30040.\nI think we should expose `tol` as in `Ridge`.", "created_at": "2024-12-20T20:41:37Z"}
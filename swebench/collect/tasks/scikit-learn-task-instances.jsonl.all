{"repo": "scikit-learn/scikit-learn", "pull_number": 30776, "instance_id": "scikit-learn__scikit-learn-30776", "issue_numbers": ["30774"], "base_commit": "91d49e86321781deb6b5b4d293905d364dffaa6f", "patch": "diff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py\nindex 6a11b758c0da5..bace298a93b67 100644\n--- a/sklearn/utils/estimator_checks.py\n+++ b/sklearn/utils/estimator_checks.py\n@@ -833,7 +833,8 @@ def callback(\n     if generate_only:\n         warnings.warn(\n             \"`generate_only` is deprecated in 1.6 and will be removed in 1.8. \"\n-            \"Use :func:`~sklearn.utils.estimator_checks.estimator_checks` instead.\",\n+            \"Use :func:`~sklearn.utils.estimator_checks.estimator_checks_generator` \"\n+            \"instead.\",\n             FutureWarning,\n         )\n         return estimator_checks_generator(\n", "test_patch": "", "problem_statement": "Deprecation message of check_estimator does not point to the right replacement\nSee here\n\nhttps://github.com/scikit-learn/scikit-learn/blob/e25e8e2119ab6c5aa5072b05c0eb60b10aee4b05/sklearn/utils/estimator_checks.py#L836\n\nI believe it should point to `sklearn.utils.estimator_checks.estimator_checks_generator` as suggested in the doc string.\n\nAlso not sure you want to keep the sphinx directive in the warning message.\n", "hints_text": "", "created_at": "2025-02-06T13:17:31Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30725, "instance_id": "scikit-learn__scikit-learn-30725", "issue_numbers": ["8958"], "base_commit": "f75917645f6821e10627c17a25987673dd154d2b", "patch": "diff --git a/doc/conf.py b/doc/conf.py\nindex 9feba868ea64f..cca6fb0da549f 100644\n--- a/doc/conf.py\n+++ b/doc/conf.py\n@@ -207,6 +207,11 @@\n # Sphinx are currently 'default' and 'sphinxdoc'.\n html_theme = \"pydata_sphinx_theme\"\n \n+# This config option is used to generate the canonical links in the header\n+# of every page. The canonical link is needed to prevent search engines from\n+# returning results pointing to old scikit-learn versions.\n+html_baseurl = \"https://scikit-learn.org/stable/\"\n+\n # Theme options are theme-specific and customize the look and feel of a theme\n # further.  For a list of options available for each theme, see the\n # documentation.\n", "test_patch": "", "problem_statement": "Make it less likely that Google is indexing old version of the docs (maybe with robots.txt)\nGoogle is indexing old versions of the docs, leading to problems such as #4736.\r\n\r\nI suggest adding a robots.txt to fix the problem. I suggest the following content:\r\n\r\n<pre>\r\nUser-agent: *\r\nDisallow: /*/ \r\nAllow: /stable\r\n</pre>\r\n\r\nI am not sure that the disallow line is correct, though :$. But I think that it is worth trying.\r\n\r\nWhat do people think?\n", "hints_text": "[Google says:](https://support.google.com/webmasters/answer/6062608?hl=en)\r\n> You should not use robots.txt as a means to hide your web pages from Google Search results\r\n\r\nInstead, it suggests to add an [html meta tag](https://developers.google.com/webmasters/control-crawl-index/docs/robots_meta_tag) in the concerned pages.\r\n```html\r\n<meta name=\"robots\" content=\"noindex\" />\r\n```\n> Instead, it suggests to add an html meta tag in the concerned pages.\n\n> <meta name=\"robots\" content=\"noindex\" />\n\nGood catch.\n\nNow, how do we do this in practice? Run a script on the git repo of the\nwebpage to add this?\n\nI'm surprised that the rel=canonical link does not already do this... as\nlong as the same path exists in stable, that is.\n\nOn 1 Jun 2017 1:17 am, \"Gael Varoquaux\" <notifications@github.com> wrote:\n\n> > Instead, it suggests to add an html meta tag in the concerned pages.\n>\n> > <meta name=\"robots\" content=\"noindex\" />\n>\n> Good catch.\n>\n> Now, how do we do this in practice? Run a script on the git repo of the\n> webpage to add this?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/8958#issuecomment-305220164>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz67MwSOjI6KdORNBwYm57NgwWilrCks5r_YSPgaJpZM4NqE4s>\n> .\n>\n\nWe have a big warning on old versions but that would still be a nice to have.\r\n\r\nWe probably need a script to insert this tag to all the pages of the old versions of the doc hosted here:\r\n\r\nhttps://github.com/scikit-learn/scikit-learn.github.io/\r\n\r\nIdeally this script would integrated into our Circle CI based documentation builder.\nI thought \"is this actually happening\"? I never remembered having such an issue but actually it does not for some search engines. Google does not do that (at least for me on the first page of results) but for example DuckDuckGo does returns gives a 0.18 example as the second match:\r\nhttps://duckduckgo.com/?q=sklearn+grid+search+digits\r\n![image](https://user-images.githubusercontent.com/1680079/115612432-89eda800-a2eb-11eb-9f89-1e9b443e9706.png)\r\n\r\nAt the same time you get greeted by a big warning (as @ogrisel was saying) so maybe good enough?\r\n![image](https://user-images.githubusercontent.com/1680079/115612071-2794a780-a2eb-11eb-8c26-84ce5c1252af.png)\r\n\n> We probably need a script to insert this tag to all the pages of the old versions of the doc hosted here:\r\n\r\nCan't we just add a robots.txt in the root folder? That's what ReadTheDocs does to hide versions.\nExample of ReadTheDocs configuration,\r\n```\r\nUser-agent: *\r\n\r\nDisallow: /en/0.17.0a2/ # Hidden version\r\n\r\nDisallow: /en/0.16.1/ # Hidden version\r\n```\nSo it looks like this is happening again and more often for 1.5 for some reason. I was looking at the website stats and plenty of 1.5 pages are in the top results:\n\n![Image](https://github.com/user-attachments/assets/9db90f6e-e843-473c-a766-fae6b3fcffbe)\n\nThis has also been reported in https://github.com/scikit-learn/scikit-learn/discussions/30672. \n\nFor me this happens with Google or https://search.brave.com but not DuckDuckGo or Qwant. This may well depend on search personalization ...\n\nWith Google the version pointed to depends on what you search sometimes `dev`, sometimes 1.5, sometimes 1.6:\n- `train_test_split` dev\n- `LogisticRegression` 1.5\n- `HistGradientBoostingClassifier` dev\n- `HistGradientBoostingRegressor` 1.5\n- `GradientBoostingClassifier` 1.5\n- `Pipeline` 1.5\n- `cross_val_score` 1.6\nWe should have some form of `rel=canonical` link in our pages to make sure search engines know which version to send people to.\n\nFrom a (small sample) look at the source of view-source:https://scikit-learn.org/1.5/auto_examples/classification/plot_lda_qda.html and view-source:https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py it seems that neither contains a `rel=canonical`.\n\nI think the correct thing to do would be to include a `<link rel=\"canonical\" href=\"https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html\" />` - so something that points to the `/stable` version\nFollowing the meeting discussion, I have one reasonable hypothesis about \"why is it happening now?\" is that there was some `rel=\"canonical\"` until 1.4 and it disappeared in 1.5, likely because of the switch to pydata-sphinx-theme?\n\nCompared https://scikit-learn.org/1.4/install.html to https://scikit-learn.org/1.5/install.html.\n\nIt was part of our custom layout until 1.4 https://github.com/scikit-learn/scikit-learn/blob/08c266da4f0c22b5dfa29c9d27510fc282590d4c/doc/themes/scikit-learn-modern/layout.html#L23. \n\nNow the question is how do we do the same thing with pydata-sphinx-theme?\n\nFor completeness, I guess the `robots.txt` may help eventually but if this simple enough to do it the proper way, why not?\nFrom https://pydata-sphinx-theme.readthedocs.io/en/stable/api/pydata_sphinx_theme/index.html#pydata_sphinx_theme._fix_canonical_url I guess that setting `html_baseurl` in the config would maybe add a canonical link back? Trying it now\nThe funny thing is that 10+ years ago in the same project other people were in a very similar situation https://github.com/scikit-learn/scikit-learn/issues/2192#issuecomment-21400681 \ud83e\udd23\n\n> The robots.txt was a mistake (my mistake). We need to change it to using link rel=\"canonical\"\n\nOh well live and learn (and then forget and go back to square 1) \ud83d\ude09", "created_at": "2025-01-27T17:26:24Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30685, "instance_id": "scikit-learn__scikit-learn-30685", "issue_numbers": ["8958"], "base_commit": "f09c7d94c54abf8c546fa6380e219f9939279bec", "patch": "diff --git a/doc/conf.py b/doc/conf.py\nindex 4a5d2a6ec9c6b..36789ae6b5aea 100644\n--- a/doc/conf.py\n+++ b/doc/conf.py\n@@ -340,7 +340,7 @@\n html_additional_pages = {\"index\": \"index.html\"}\n \n # Additional files to copy\n-# html_extra_path = []\n+html_extra_path = [\"robots.txt\"]\n \n # Additional JS files\n html_js_files = [\ndiff --git a/doc/robots.txt b/doc/robots.txt\nnew file mode 100644\nindex 0000000000000..10d0ec9c16677\n--- /dev/null\n+++ b/doc/robots.txt\n@@ -0,0 +1,4 @@\n+User-agent: *\n+Disallow: /\n+Allow: /stable\n+Allow: /dev/developers\n", "test_patch": "", "problem_statement": "Make it less likely that Google is indexing old version of the docs (maybe with robots.txt)\nGoogle is indexing old versions of the docs, leading to problems such as #4736.\r\n\r\nI suggest adding a robots.txt to fix the problem. I suggest the following content:\r\n\r\n<pre>\r\nUser-agent: *\r\nDisallow: /*/ \r\nAllow: /stable\r\n</pre>\r\n\r\nI am not sure that the disallow line is correct, though :$. But I think that it is worth trying.\r\n\r\nWhat do people think?\n", "hints_text": "[Google says:](https://support.google.com/webmasters/answer/6062608?hl=en)\r\n> You should not use robots.txt as a means to hide your web pages from Google Search results\r\n\r\nInstead, it suggests to add an [html meta tag](https://developers.google.com/webmasters/control-crawl-index/docs/robots_meta_tag) in the concerned pages.\r\n```html\r\n<meta name=\"robots\" content=\"noindex\" />\r\n```\n> Instead, it suggests to add an html meta tag in the concerned pages.\n\n> <meta name=\"robots\" content=\"noindex\" />\n\nGood catch.\n\nNow, how do we do this in practice? Run a script on the git repo of the\nwebpage to add this?\n\nI'm surprised that the rel=canonical link does not already do this... as\nlong as the same path exists in stable, that is.\n\nOn 1 Jun 2017 1:17 am, \"Gael Varoquaux\" <notifications@github.com> wrote:\n\n> > Instead, it suggests to add an html meta tag in the concerned pages.\n>\n> > <meta name=\"robots\" content=\"noindex\" />\n>\n> Good catch.\n>\n> Now, how do we do this in practice? Run a script on the git repo of the\n> webpage to add this?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/scikit-learn/scikit-learn/issues/8958#issuecomment-305220164>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEz67MwSOjI6KdORNBwYm57NgwWilrCks5r_YSPgaJpZM4NqE4s>\n> .\n>\n\nWe have a big warning on old versions but that would still be a nice to have.\r\n\r\nWe probably need a script to insert this tag to all the pages of the old versions of the doc hosted here:\r\n\r\nhttps://github.com/scikit-learn/scikit-learn.github.io/\r\n\r\nIdeally this script would integrated into our Circle CI based documentation builder.\nI thought \"is this actually happening\"? I never remembered having such an issue but actually it does not for some search engines. Google does not do that (at least for me on the first page of results) but for example DuckDuckGo does returns gives a 0.18 example as the second match:\r\nhttps://duckduckgo.com/?q=sklearn+grid+search+digits\r\n![image](https://user-images.githubusercontent.com/1680079/115612432-89eda800-a2eb-11eb-9f89-1e9b443e9706.png)\r\n\r\nAt the same time you get greeted by a big warning (as @ogrisel was saying) so maybe good enough?\r\n![image](https://user-images.githubusercontent.com/1680079/115612071-2794a780-a2eb-11eb-8c26-84ce5c1252af.png)\r\n\n> We probably need a script to insert this tag to all the pages of the old versions of the doc hosted here:\r\n\r\nCan't we just add a robots.txt in the root folder? That's what ReadTheDocs does to hide versions.\nExample of ReadTheDocs configuration,\r\n```\r\nUser-agent: *\r\n\r\nDisallow: /en/0.17.0a2/ # Hidden version\r\n\r\nDisallow: /en/0.16.1/ # Hidden version\r\n```\nSo it looks like this is happening again and more often for 1.5 for some reason. I was looking at the website stats and plenty of 1.5 pages are in the top results:\n\n![Image](https://github.com/user-attachments/assets/9db90f6e-e843-473c-a766-fae6b3fcffbe)\n\nThis has also been reported in https://github.com/scikit-learn/scikit-learn/discussions/30672. \n\nFor me this happens with Google or https://search.brave.com but not DuckDuckGo or Qwant. This may well depend on search personalization ...\n\nWith Google the version pointed to depends on what you search sometimes `dev`, sometimes 1.5, sometimes 1.6:\n- `train_test_split` dev\n- `LogisticRegression` 1.5\n- `HistGradientBoostingClassifier` dev\n- `HistGradientBoostingRegressor` 1.5\n- `GradientBoostingClassifier` 1.5\n- `Pipeline` 1.5\n- `cross_val_score` 1.6", "created_at": "2025-01-21T04:21:36Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30671, "instance_id": "scikit-learn__scikit-learn-30671", "issue_numbers": ["30648"], "base_commit": "f09c7d94c54abf8c546fa6380e219f9939279bec", "patch": "diff --git a/sklearn/cluster/_agglomerative.py b/sklearn/cluster/_agglomerative.py\nindex 2fa7253e665b8..97d05d7dfd82f 100644\n--- a/sklearn/cluster/_agglomerative.py\n+++ b/sklearn/cluster/_agglomerative.py\n@@ -34,9 +34,7 @@\n )\n from ..utils.graph import _fix_connected_components\n from ..utils.validation import check_memory, validate_data\n-\n-# mypy error: Module 'sklearn.cluster' has no attribute '_hierarchical_fast'\n-from . import _hierarchical_fast as _hierarchical  # type: ignore\n+from . import _hierarchical_fast as _hierarchical\n from ._feature_agglomeration import AgglomerationTransform\n \n ###############################################################################\ndiff --git a/sklearn/linear_model/_least_angle.py b/sklearn/linear_model/_least_angle.py\nindex 25f956e5fadda..7471eda67cccd 100644\n--- a/sklearn/linear_model/_least_angle.py\n+++ b/sklearn/linear_model/_least_angle.py\n@@ -18,9 +18,7 @@\n from ..base import MultiOutputMixin, RegressorMixin, _fit_context\n from ..exceptions import ConvergenceWarning\n from ..model_selection import check_cv\n-\n-# mypy error: Module 'sklearn.utils' has no attribute 'arrayfuncs'\n-from ..utils import (  # type: ignore\n+from ..utils import (\n     Bunch,\n     arrayfuncs,\n     as_float_array,\ndiff --git a/sklearn/manifold/_t_sne.py b/sklearn/manifold/_t_sne.py\nindex 71125d8b9f1d5..fd9277dabd5e9 100644\n--- a/sklearn/manifold/_t_sne.py\n+++ b/sklearn/manifold/_t_sne.py\n@@ -29,7 +29,6 @@\n from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\n from ..utils.validation import _num_samples, check_non_negative, validate_data\n \n-# mypy error: Module 'sklearn.manifold' has no attribute '_utils'\n # mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne'\n from . import _barnes_hut_tsne, _utils  # type: ignore\n \n", "test_patch": "", "problem_statement": "MNT: Remove ignore comments for mypy as we use `ignore_missing_imports`\nWe set:\n\nhttps://github.com/scikit-learn/scikit-learn/blob/2707099b23a0a8580731553629566c1182d26f48/setup.cfg#L21\n\nBut in many places we add a comment and an ignore for these, see: https://github.com/search?q=repo%3Ascikit-learn%2Fscikit-learn+mypy+error&type=code\n\nShould we remove these comments? (or do we think it's useful to leave in?)\n", "hints_text": "I'm not familiar enough with mypy to be sure that `ignore_missing_imports = True` is enough to make it not complain, but if you try and the CI is green I'd see no reason to keep local directives.", "created_at": "2025-01-18T22:49:33Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30630, "instance_id": "scikit-learn__scikit-learn-30630", "issue_numbers": ["30623"], "base_commit": "36ad7b3ec16d141ef164e5cb7da29247a656bbb5", "patch": "diff --git a/examples/ensemble/plot_gradient_boosting_quantile.py b/examples/ensemble/plot_gradient_boosting_quantile.py\nindex 3e2c44568de3c..60b6b24c3724e 100644\n--- a/examples/ensemble/plot_gradient_boosting_quantile.py\n+++ b/examples/ensemble/plot_gradient_boosting_quantile.py\n@@ -104,12 +104,10 @@ def f(x):\n y_med = all_models[\"q 0.50\"].predict(xx)\n \n fig = plt.figure(figsize=(10, 10))\n-plt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n+plt.plot(xx, f(xx), \"black\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n plt.plot(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\n-plt.plot(xx, y_med, \"r-\", label=\"Predicted median\")\n-plt.plot(xx, y_pred, \"r-\", label=\"Predicted mean\")\n-plt.plot(xx, y_upper, \"k-\")\n-plt.plot(xx, y_lower, \"k-\")\n+plt.plot(xx, y_med, \"tab:orange\", linewidth=3, label=\"Predicted median\")\n+plt.plot(xx, y_pred, \"tab:green\", linewidth=3, label=\"Predicted mean\")\n plt.fill_between(\n     xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\n )\n@@ -310,10 +308,8 @@ def coverage_fraction(y, y_low, y_high):\n y_upper = search_95p.predict(xx)\n \n fig = plt.figure(figsize=(10, 10))\n-plt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n+plt.plot(xx, f(xx), \"black\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n plt.plot(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\n-plt.plot(xx, y_upper, \"k-\")\n-plt.plot(xx, y_lower, \"k-\")\n plt.fill_between(\n     xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\n )\n", "test_patch": "", "problem_statement": "Bad color choice in Prediction Intervals for Gradient Boosting Regression\n### Describe the issue linked to the documentation\r\n\r\nThe first plot in the example [Prediction Intervals for Gradient Boosting Regression](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html#fitting-non-linear-quantile-and-least-squares-regressors) is plotting mean and median using orange and red (I think? maybe it's even the exact same color) which makes it near-impossible to differentiate, at least on my device.\r\n\r\n![sphx_glr_plot_gradient_boosting_quantile_001](https://github.com/user-attachments/assets/4b4fbbbe-e042-46ac-bd54-895d72c3d9a1)\r\n\r\n### Suggest a potential alternative/fix\r\n\r\nEDIT: As Julian points out, the color actually is being set explicitly, my bad.\r\n\r\nIn the code, no color is being set explicitly (the relevant lines are 109 and 110 [here](https://github.com/scikit-learn/scikit-learn/blob/main/examples/ensemble/plot_gradient_boosting_quantile.py#L109)).\r\nHonestly, I think explicitly setting more or less any other color for either the mean or the median would work better, like purple or even yellow (due to the blue shading in the background).\r\n\n", "hints_text": "@MischaDy it seems that both [Line 109]( https://github.com/scikit-learn/scikit-learn/blob/5691f2672a4d5bc0ce36629aec58d8a4076e5e99/examples/ensemble/plot_gradient_boosting_quantile.py#L109) and [Line 110](https://github.com/scikit-learn/scikit-learn/blob/5691f2672a4d5bc0ce36629aec58d8a4076e5e99/examples/ensemble/plot_gradient_boosting_quantile.py#L110) explicitly set both the plots for the predicted mean and median to red. Yes, I do agree with @MischaDy that literally any other colour would be useful to distinguish them here.\nI'd like to work on this issue. I plan to change one of the line colors to make the mean and median lines more distinguishable.", "created_at": "2025-01-13T00:02:28Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30595, "instance_id": "scikit-learn__scikit-learn-30595", "issue_numbers": ["30594"], "base_commit": "e25e8e2119ab6c5aa5072b05c0eb60b10aee4b05", "patch": "diff --git a/sklearn/model_selection/_split.py b/sklearn/model_selection/_split.py\nindex 5446a7b3f9cbf..e4759c14e4ad5 100644\n--- a/sklearn/model_selection/_split.py\n+++ b/sklearn/model_selection/_split.py\n@@ -2865,6 +2865,56 @@ def train_test_split(\n \n     >>> train_test_split(y, shuffle=False)\n     [[0, 1, 2], [3, 4]]\n+\n+    >>> from sklearn import datasets\n+    >>> iris = datasets.load_iris(as_frame=True)\n+    >>> X, y = iris['data'], iris['target']\n+    >>> X.head()\n+        sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n+    0                5.1               3.5                1.4               0.2\n+    1                4.9               3.0                1.4               0.2\n+    2                4.7               3.2                1.3               0.2\n+    3                4.6               3.1                1.5               0.2\n+    4                5.0               3.6                1.4               0.2\n+    >>> y.head()\n+    0    0\n+    1    0\n+    2    0\n+    3    0\n+    4    0\n+    ...\n+\n+    >>> X_train, X_test, y_train, y_test = train_test_split(\n+    ... X, y, test_size=0.33, random_state=42)\n+    ...\n+    >>> X_train.head()\n+        sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n+    96                 5.7               2.9                4.2               1.3\n+    105                7.6               3.0                6.6               2.1\n+    66                 5.6               3.0                4.5               1.5\n+    0                  5.1               3.5                1.4               0.2\n+    122                7.7               2.8                6.7               2.0\n+    >>> y_train.head()\n+    96     1\n+    105    2\n+    66     1\n+    0      0\n+    122    2\n+    ...\n+    >>> X_test.head()\n+        sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n+    73                 6.1               2.8                4.7               1.2\n+    18                 5.7               3.8                1.7               0.3\n+    118                7.7               2.6                6.9               2.3\n+    78                 6.0               2.9                4.5               1.5\n+    76                 6.8               2.8                4.8               1.4\n+    >>> y_test.head()\n+    73     1\n+    18     0\n+    118    2\n+    78     1\n+    76     1\n+    ...\n     \"\"\"\n     n_arrays = len(arrays)\n     if n_arrays == 0:\n", "test_patch": "", "problem_statement": "DOC: Example of `train_test_split` with `pandas` DataFrames\n### Describe the issue linked to the documentation\r\n\r\nCurrently, the example [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) only illustrates the use case of `train_test_split` for `numpy` arrays. I think an additional example featuring a `pandas` DataFrame would  make this page more beginner-friendly. Would you guys be interested? \r\n\r\n### Suggest a potential alternative/fix\r\n\r\nThe modification in [`model_selection/_split`](https://github.com/scikit-learn/scikit-learn/blob/d666202a9349893c1bd106cc9ee0ff0a807c7cf3/sklearn/model_selection/_split.py) would be the following:\r\n```\r\n\"\"\"\r\nExample: Data are a `numpy` array\r\n--------\r\n>>> Current example\r\n\r\nExample: Data are a `pandas` DataFrame\r\n--------\r\n>>> from sklearn import datasets\r\n>>> from sklearn.model_selection import train_test_split\r\n>>> iris = datasets.load_iris(as_frame=True)\r\n>>> X, y = iris['data'], iris['target']\r\n>>> X.head()\r\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\r\n0                5.1               3.5                1.4               0.2\r\n1                4.9               3.0                1.4               0.2\r\n2                4.7               3.2                1.3               0.2\r\n3                4.6               3.1                1.5               0.2\r\n4                5.0               3.6                1.4               0.2\r\n>>> y.head()\r\n0    0\r\n1    0\r\n2    0\r\n3    0\r\n4    0\r\n>>> X_train, X_test, y_train, y_test = train_test_split(\r\n... X, y, test_size=0.33, random_state=42) # rows will be shuffled\r\n>>> X_train.head()\r\n     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\r\n96                 5.7               2.9                4.2               1.3\r\n105                7.6               3.0                6.6               2.1\r\n66                 5.6               3.0                4.5               1.5\r\n0                  5.1               3.5                1.4               0.2\r\n122                7.7               2.8                6.7               2.0\r\n>>> y_train\r\n96     1\r\n105    2\r\n66     1\r\n0      0\r\n122    2\r\n>>> X_test.head()\r\n     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\r\n73                 6.1               2.8                4.7               1.2\r\n18                 5.7               3.8                1.7               0.3\r\n118                7.7               2.6                6.9               2.3\r\n78                 6.0               2.9                4.5               1.5\r\n76                 6.8               2.8                4.8               1.4\r\n>>> y_test.head()\r\n73     1\r\n18     0\r\n118    2\r\n78     1\r\n76     1\r\n\"\"\"\r\n```\n", "hints_text": "", "created_at": "2025-01-06T13:44:58Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30550, "instance_id": "scikit-learn__scikit-learn-30550", "issue_numbers": ["30541"], "base_commit": "970503f839f44b4f78390e6069f8e13c0dd2f185", "patch": "diff --git a/doc/glossary.rst b/doc/glossary.rst\nindex a5feb72a268f4..4319cb38878cb 100644\n--- a/doc/glossary.rst\n+++ b/doc/glossary.rst\n@@ -1793,7 +1793,7 @@ See concept :term:`attribute`.\n         the number of output features and :term:`n_features` is the number of\n         input features.\n \n-        See also :term:`components_` which is a similar attribute for linear\n+        See also :term:`coef_` which is a similar attribute for linear\n         predictors.\n \n     ``coef_``\n", "test_patch": "", "problem_statement": "Glossary: See-also for `components_` attribute references itself\n### Describe the issue linked to the documentation\n\nAt <https://scikit-learn.org/stable/glossary.html#term-components_>, the `components_` entry references itself:\r\n\r\n> See also [components_](https://scikit-learn.org/stable/glossary.html#term-components_) which is a similar attribute for linear predictors.\r\n\r\nIs this mean to refer to `coef_` (the next item) instead of itself (`components_`) again?\n\n### Suggest a potential alternative/fix\n\n_No response_\n", "hints_text": "I agree with you, there seems to be an error here, and I believe the `coef_` attribute is intended to be referenced.\r\nIf it suits you, I would like to work on this issue.\r\nI have never contributed to scikit-learn and would like to accommodate myself with pull request procedures by working on a simple issue.\n@ArthurDbrn feel free to go ahead an open a pull request with your fix in `doc/glossary.rst`.\nIf it's your first PR, you might want to have a look at:\r\n\r\nhttps://scikit-learn.org/dev/developers/contributing.html\n@ogrisel Thank you I'll get to work then.\n/take", "created_at": "2024-12-27T22:45:58Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30521, "instance_id": "scikit-learn__scikit-learn-30521", "issue_numbers": ["30131"], "base_commit": "eefcb113a410cc7cb15b16ebbbec3156832f8fdb", "patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.linear_model/30521.fix.rst b/doc/whats_new/upcoming_changes/sklearn.linear_model/30521.fix.rst\nnew file mode 100644\nindex 0000000000000..537c3760b16df\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.linear_model/30521.fix.rst\n@@ -0,0 +1,4 @@\n+-  |Enhancement| Added a new paramenter `tol` to\n+   :class:`linear_model.LinearRegression` that determines the precision of the\n+   solution `coef_` when fitting on sparse data. :pr:`30521` by :user:`Success Moses\n+   <SuccessMoses>`.\ndiff --git a/sklearn/linear_model/_base.py b/sklearn/linear_model/_base.py\nindex bb71cbe9ed550..6aa2e3e6563dc 100644\n--- a/sklearn/linear_model/_base.py\n+++ b/sklearn/linear_model/_base.py\n@@ -8,7 +8,7 @@\n import numbers\n import warnings\n from abc import ABCMeta, abstractmethod\n-from numbers import Integral\n+from numbers import Integral, Real\n \n import numpy as np\n import scipy.sparse as sp\n@@ -32,6 +32,7 @@\n     indexing_dtype,\n     supported_float_dtypes,\n )\n+from ..utils._param_validation import Interval\n from ..utils._seq_dataset import (\n     ArrayDataset32,\n     ArrayDataset64,\n@@ -472,6 +473,15 @@ class LinearRegression(MultiOutputMixin, RegressorMixin, LinearModel):\n     copy_X : bool, default=True\n         If True, X will be copied; else, it may be overwritten.\n \n+    tol : float, default=1e-4\n+        The precision of the solution (`coef_`) is determined by `tol` which\n+        specifies a different convergence criterion for the `lsqr` solver.\n+        `tol` is set as `atol` and `btol` of `scipy.sparse.linalg.lsqr` when\n+        fitting on sparse training data. This parameter has no effect when fitting\n+        on dense data.\n+\n+        .. versionadded:: 1.7\n+\n     n_jobs : int, default=None\n         The number of jobs to use for the computation. This will only provide\n         speedup in case of sufficiently large problems, that is if firstly\n@@ -555,6 +565,7 @@ class LinearRegression(MultiOutputMixin, RegressorMixin, LinearModel):\n         \"copy_X\": [\"boolean\"],\n         \"n_jobs\": [None, Integral],\n         \"positive\": [\"boolean\"],\n+        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n     }\n \n     def __init__(\n@@ -562,11 +573,13 @@ def __init__(\n         *,\n         fit_intercept=True,\n         copy_X=True,\n+        tol=1e-4,\n         n_jobs=None,\n         positive=False,\n     ):\n         self.fit_intercept = fit_intercept\n         self.copy_X = copy_X\n+        self.tol = tol\n         self.n_jobs = n_jobs\n         self.positive = positive\n \n@@ -668,11 +681,13 @@ def rmatvec(b):\n             )\n \n             if y.ndim < 2:\n-                self.coef_ = lsqr(X_centered, y)[0]\n+                self.coef_ = lsqr(X_centered, y, atol=self.tol, btol=self.tol)[0]\n             else:\n                 # sparse_lstsq cannot handle y with shape (M, K)\n                 outs = Parallel(n_jobs=n_jobs_)(\n-                    delayed(lsqr)(X_centered, y[:, j].ravel())\n+                    delayed(lsqr)(\n+                        X_centered, y[:, j].ravel(), atol=self.tol, btol=self.tol\n+                    )\n                     for j in range(y.shape[1])\n                 )\n                 self.coef_ = np.vstack([out[0] for out in outs])\n", "test_patch": "diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py\nindex d7a201f3abf6f..98f3ab21b9e16 100644\n--- a/sklearn/tests/test_pipeline.py\n+++ b/sklearn/tests/test_pipeline.py\n@@ -371,7 +371,7 @@ def test_pipeline_raise_set_params_error():\n     # expected error message for invalid inner parameter\n     error_msg = re.escape(\n         \"Invalid parameter 'invalid_param' for estimator LinearRegression(). Valid\"\n-        \" parameters are: ['copy_X', 'fit_intercept', 'n_jobs', 'positive'].\"\n+        \" parameters are: ['copy_X', 'fit_intercept', 'n_jobs', 'positive', 'tol'].\"\n     )\n     with pytest.raises(ValueError, match=error_msg):\n         pipe.set_params(cls__invalid_param=\"nope\")\ndiff --git a/sklearn/utils/_test_common/instance_generator.py b/sklearn/utils/_test_common/instance_generator.py\nindex bac401d8d657f..c46213b417090 100644\n--- a/sklearn/utils/_test_common/instance_generator.py\n+++ b/sklearn/utils/_test_common/instance_generator.py\n@@ -575,6 +575,7 @@\n             dict(positive=False),\n             dict(positive=True),\n         ],\n+        \"check_sample_weight_equivalence_on_sparse_data\": [dict(tol=1e-12)],\n     },\n     LocallyLinearEmbedding: {\"check_dict_unchanged\": dict(max_iter=5, n_components=1)},\n     LogisticRegression: {\n@@ -983,18 +984,6 @@ def _yield_instances_for_check(check, estimator_orig):\n     KNeighborsTransformer: {\n         \"check_methods_sample_order_invariance\": \"check is not applicable.\"\n     },\n-    LinearRegression: {\n-        # TODO: this model should converge to the minimum norm solution of the\n-        # least squares problem and as result be numerically stable enough when\n-        # running the equivalence check even if n_features > n_samples. Maybe\n-        # this is is not the case and a different choice of solver could fix\n-        # this problem. This might require setting a low enough value for the\n-        # tolerance of the lsqr solver:\n-        # https://github.com/scikit-learn/scikit-learn/issues/30131\n-        \"check_sample_weight_equivalence_on_sparse_data\": (\n-            \"sample_weight is not equivalent to removing/repeating samples.\"\n-        ),\n-    },\n     LinearSVC: {\n         # TODO: replace by a statistical test when _dual=True, see meta-issue #16298\n         \"check_sample_weight_equivalence_on_dense_data\": (\n", "problem_statement": "LinearRegression on sparse matrices is not sample weight consistent\nPart of #16298.\r\n\r\n### Describe the bug\r\n\r\nWhen using a sparse container like `csr_array` for `X`, `LinearRegression` even fails to give the same coefficients for unit or no sample weight, and more generally fails the `test_linear_regression_sample_weight_consitency` checks. In that setting, the underlying solver is `scipy.sparse.linalg.lsqr`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.utils.fixes import csr_array\r\nfrom sklearn.datasets import make_regression\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.utils._testing import assert_allclose\r\n\r\nX, y = make_regression(100, 100, random_state=42)\r\nX = csr_array(X)\r\nreg = LinearRegression(fit_intercept=True)\r\nreg.fit(X, y)\r\ncoef1 = reg.coef_\r\nreg.fit(X, y, sample_weight=np.ones_like(y))\r\ncoef2 = reg.coef_\r\nassert_allclose(coef1, coef2, rtol=1e-7, atol=1e-9)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe `assert_allclose` should pass.\r\n\r\n### Actual Results\r\n\r\n```Python Traceback\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=1e-09\r\n\r\nMismatched elements: 100 / 100 (100%)\r\nMax absolute difference among violations: 0.00165048\r\nMax relative difference among violations: 0.02621317\r\n ACTUAL: array([-2.450778e-01,  2.917985e+01,  1.678916e+00,  7.534454e+01,\r\n        1.241587e+01,  1.076716e+00, -4.975206e-01, -9.262295e-01,\r\n       -1.373931e+00, -1.624112e-01, -8.644422e-01, -5.986218e-01,...\r\n DESIRED: array([-2.452359e-01,  2.918078e+01,  1.678681e+00,  7.534410e+01,\r\n        1.241459e+01,  1.076624e+00, -4.962305e-01, -9.257701e-01,\r\n       -1.373862e+00, -1.622824e-01, -8.652183e-01, -5.981715e-01,...\r\n```\r\n\r\nThe test also fails for `fit_intercept=False`. Note that this test and other sample weight consistency checks pass if we do not wrap `X` in a sparse container.\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:32:50) [Clang 16.0.6 ]\r\nexecutable: /Users/abaker/miniforge3/envs/sklearn-dev/bin/python\r\n   machine: macOS-14.5-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.dev0\r\n          pip: 24.2\r\n   setuptools: 73.0.1\r\n        numpy: 2.1.0\r\n        scipy: 1.14.1\r\n       Cython: 3.0.11\r\n       pandas: 2.2.2\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libopenblas.0.dylib\r\n        version: 0.3.27\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libomp\r\n       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libomp.dylib\r\n        version: None\r\n```\r\n\r\nEDIT: discovered while working on #30040 for the case of dense inputs.\n", "hints_text": "Note, that this is not the case for `Ridge(solver=\"lsqr\", alpha=0, tol=1e-12)`:\r\n\r\n```python\r\n>>> from sklearn.utils.fixes import csr_array\r\n... from sklearn.datasets import make_regression\r\n... from sklearn.linear_model import Ridge\r\n... from sklearn.utils._testing import assert_allclose\r\n... \r\n... X, y = make_regression(100, 100, random_state=42)\r\n... X = csr_array(X)\r\n... reg = Ridge(solver=\"lsqr\", alpha=0, fit_intercept=True, tol=1e-12)\r\n... reg.fit(X, y)\r\n... coef1 = reg.coef_\r\n... reg.fit(X, y, sample_weight=np.ones_like(y))\r\n... coef2 = reg.coef_\r\n... assert_allclose(coef1, coef2, rtol=1e-7, atol=1e-9)\r\n```\r\n\r\nBut the same fails with a larger `tol` value, which is passed as `atol` & `btol` parameters to the `lsqr` call:\r\n\r\n```python\r\n>>> reg = Ridge(solver=\"lsqr\", alpha=0, fit_intercept=True, tol=1e-4)\r\n... reg.fit(X, y)\r\n... coef1 = reg.coef_\r\n... reg.fit(X, y, sample_weight=np.ones_like(y))\r\n... coef2 = reg.coef_\r\n... assert_allclose(coef1, coef2, rtol=1e-7, atol=1e-9)\r\nTraceback (most recent call last):\r\n  Cell In[9], line 13\r\n    assert_allclose(coef1, coef2, rtol=1e-7, atol=1e-9)\r\n  File ~/code/scikit-learn/sklearn/utils/_testing.py:232 in assert_allclose\r\n    np_assert_allclose(\r\n  File ~/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py:1688 in assert_allclose\r\n    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\r\n  File ~/miniforge3/envs/dev/lib/python3.12/contextlib.py:81 in inner\r\n    return func(*args, **kwds)\r\n  File ~/miniforge3/envs/dev/lib/python3.12/site-packages/numpy/testing/_private/utils.py:889 in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=1e-09\r\n\r\nMismatched elements: 100 / 100 (100%)\r\nMax absolute difference among violations: 0.00143505\r\nMax relative difference among violations: 0.03345462\r\n ACTUAL: array([ 5.135931e-01,  3.037594e+01,  2.145667e+00,  7.394602e+01,\r\n        1.324545e+01, -4.438229e-02, -1.202226e+00,  7.553014e-01,\r\n       -1.661572e+00,  1.483041e-01, -3.624484e-01, -1.915631e+00,...\r\n DESIRED: array([ 5.125267e-01,  3.037609e+01,  2.145875e+00,  7.394601e+01,\r\n        1.324581e+01, -4.334871e-02, -1.202492e+00,  7.555202e-01,\r\n       -1.661108e+00,  1.495079e-01, -3.613935e-01, -1.916570e+00,...\r\n```\r\n\r\nI think we need to pass `atol=self.tol` / `btol=self.tol` to the inner call to `lsqr` and expose a tolerance parameter as we do in ridge. Or alternatively set a default strict, `dtype`-dependent value as we now do for the `cond` parameter in #30040.\nI think we should expose `tol` as in `Ridge`.", "created_at": "2024-12-20T20:41:37Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30516, "instance_id": "scikit-learn__scikit-learn-30516", "issue_numbers": ["30479"], "base_commit": "72b35a46684c0ecf4182500d3320836607d1f17c", "patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.utils/30516.fix.rst b/doc/whats_new/upcoming_changes/sklearn.utils/30516.fix.rst\nnew file mode 100644\nindex 0000000000000..6e008f3beeb3c\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.utils/30516.fix.rst\n@@ -0,0 +1,4 @@\n+- Raise a `DeprecationWarning` when there is no concrete implementation of `__sklearn_tags__`\n+  in the MRO of the estimator. We request to inherit from `BaseEstimator` that\n+  implements `__sklearn_tags__`.\n+  By :user:`Guillaume Lemaitre <glemaitre>`\n\\ No newline at end of file\ndiff --git a/sklearn/utils/_tags.py b/sklearn/utils/_tags.py\nindex d4f211eb52152..408058ca15096 100644\n--- a/sklearn/utils/_tags.py\n+++ b/sklearn/utils/_tags.py\n@@ -393,7 +393,32 @@ def get_tags(estimator) -> Tags:\n     tag_provider = _find_tags_provider(estimator)\n \n     if tag_provider == \"__sklearn_tags__\":\n-        tags = estimator.__sklearn_tags__()\n+        # TODO(1.7): turn the warning into an error\n+        try:\n+            tags = estimator.__sklearn_tags__()\n+        except AttributeError as exc:\n+            if str(exc) == \"'super' object has no attribute '__sklearn_tags__'\":\n+                # workaround the regression reported in\n+                # https://github.com/scikit-learn/scikit-learn/issues/30479\n+                # `__sklearn_tags__` is implemented by calling\n+                # `super().__sklearn_tags__()` but there is no `__sklearn_tags__`\n+                # method in the base class.\n+                warnings.warn(\n+                    f\"The following error was raised: {str(exc)}. It seems that \"\n+                    \"there are no classes that implement `__sklearn_tags__` \"\n+                    \"in the MRO and/or all classes in the MRO call \"\n+                    \"`super().__sklearn_tags__()`. Make sure to inherit from \"\n+                    \"`BaseEstimator` which implements `__sklearn_tags__` (or \"\n+                    \"alternatively define `__sklearn_tags__` but we don't recommend \"\n+                    \"this approach). Note that `BaseEstimator` needs to be on the \"\n+                    \"right side of other Mixins in the inheritance order. The \"\n+                    \"default are now used instead since retrieving tags failed. \"\n+                    \"This warning will be replaced by an error in 1.7.\",\n+                    category=DeprecationWarning,\n+                )\n+                tags = default_tags(estimator)\n+            else:\n+                raise\n     else:\n         # TODO(1.7): Remove this branch of the code\n         # Let's go through the MRO and patch each class implementing _more_tags\n", "test_patch": "diff --git a/sklearn/utils/tests/test_tags.py b/sklearn/utils/tests/test_tags.py\nindex 2ff6878d974fb..0a28d2dda7cf7 100644\n--- a/sklearn/utils/tests/test_tags.py\n+++ b/sklearn/utils/tests/test_tags.py\n@@ -1,12 +1,15 @@\n from dataclasses import dataclass, fields\n \n+import numpy as np\n import pytest\n \n from sklearn.base import (\n     BaseEstimator,\n+    ClassifierMixin,\n     RegressorMixin,\n     TransformerMixin,\n )\n+from sklearn.pipeline import Pipeline\n from sklearn.utils import (\n     ClassifierTags,\n     InputTags,\n@@ -629,3 +632,48 @@ def __sklearn_tags__(self):\n     }\n     assert old_tags == expected_tags\n     assert _to_new_tags(_to_old_tags(new_tags), estimator=estimator) == new_tags\n+\n+\n+# TODO(1.7): Remove this test\n+def test_tags_no_sklearn_tags_concrete_implementation():\n+    \"\"\"Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/30479\n+\n+    There is no class implementing `__sklearn_tags__` without calling\n+    `super().__sklearn_tags__()`. Thus, we raise a warning and request to inherit from\n+    `BaseEstimator` that implements `__sklearn_tags__`.\n+    \"\"\"\n+\n+    class MyEstimator(ClassifierMixin):\n+        def __init__(self, *, param=1):\n+            self.param = param\n+\n+        def fit(self, X, y=None):\n+            self.is_fitted_ = True\n+            return self\n+\n+        def predict(self, X):\n+            return np.full(shape=X.shape[0], fill_value=self.param)\n+\n+    X = np.array([[1, 2], [2, 3], [3, 4]])\n+    y = np.array([1, 0, 1])\n+\n+    my_pipeline = Pipeline([(\"estimator\", MyEstimator(param=1))])\n+    with pytest.warns(DeprecationWarning, match=\"The following error was raised\"):\n+        my_pipeline.fit(X, y).predict(X)\n+\n+    # check that we still raise an error if it is not a AttributeError or related to\n+    # __sklearn_tags__\n+    class MyEstimator2(MyEstimator, BaseEstimator):\n+        def __init__(self, *, param=1, error_type=AttributeError):\n+            self.param = param\n+            self.error_type = error_type\n+\n+        def __sklearn_tags__(self):\n+            super().__sklearn_tags__()\n+            raise self.error_type(\"test\")\n+\n+    for error_type in (AttributeError, TypeError, ValueError):\n+        estimator = MyEstimator2(param=1, error_type=error_type)\n+        with pytest.raises(error_type):\n+            get_tags(estimator)\n", "problem_statement": "Version 1.6.X: ClassifierMixIn failing with new __sklearn_tags__ function\n### Describe the bug\r\n\r\nHi,\r\n\r\nwe are using Sklearn in our projects for different classification training methods on production level. In the dev stage we upgraded to the latest release and our Training failed due to changes in the ClassifierMixIn Class. We use it in combination with a sklearn Pipeline.\r\n\r\nin 1.6.X the following function was introduced:\r\n\r\n```\r\n    def __sklearn_tags__(self):\r\n        tags = super().__sklearn_tags__()\r\n        tags.estimator_type = \"classifier\"\r\n        tags.classifier_tags = ClassifierTags()\r\n        tags.target_tags.required = True\r\n        return tags\r\n```\r\n\r\nIt is calling the sklearn_tags methods from it's parent class. But the ClassifierMixIn doesn't have a parent class. So it says function super().__sklearn_tags__() is not existing.\r\n\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.base import ClassifierMixin,\r\nfrom sklearn.pipeline import Pipeline\r\nimport numpy as np\r\n\r\nclass MyEstimator(ClassifierMixin):\r\n    def __init__(self, *, param=1):\r\n        self.param = param\r\n    def fit(self, X, y=None):\r\n        self.is_fitted_ = True\r\n        return self\r\n    def predict(self, X):\r\n        return np.full(shape=X.shape[0], fill_value=self.param)\r\n\r\nX = np.array([[1, 2], [2, 3], [3, 4]])\r\ny = np.array([1, 0, 1])\r\n\r\n\r\nmy_pipeline = Pipeline([(\"estimator\", MyEstimator(param=1))])\r\nmy_pipeline.fit(X, y)\r\nmy_pipeline.predict(X)\r\n```\r\n\r\n### Expected Results\r\n\r\nA Prediction is returned.\r\n\r\n### Actual Results\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\xxxx\\error_sklearn\\redo_error.py\", line 22, in <module>\r\n    my_pipeline.predict(X)\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 780, in predict\r\n    with _raise_or_warn_if_not_fitted(self):\r\n  File \"C:\\Program Files\\Wpy64-31230\\python-3.12.3.amd64\\Lib\\contextlib.py\", line 144, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 60, in _raise_or_warn_if_not_fitted\r\n    check_is_fitted(estimator)\r\n  File \"C:\\Users\\xxxx\\git_projects\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1756, in check_is_fitted\r\n    if not _is_fitted(estimator, attributes, all_or_any):\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1665, in _is_fitted\r\n    return estimator.__sklearn_is_fitted__()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in __sklearn_is_fitted__\r\n    check_is_fitted(last_step)\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1751, in check_is_fitted\r\n    tags = get_tags(estimator)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\utils\\_tags.py\", line 396, in get_tags\r\n    tags = estimator.__sklearn_tags__()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\xxxx\\error_sklearn\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 540, in __sklearn_tags__\r\n    tags = super().__sklearn_tags__()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'super' object has no attribute '__sklearn_tags__'\r\n```\r\n### Versions\r\n\r\n```shell\r\n1.6.0 / 1.6.X\r\n```\r\n\n", "hints_text": "@DaMuBo According to [the official documentation](https://scikit-learn.org/1.5/developers/develop.html#rolling-your-own-estimator), I think your `MyEstimator` should inherit from `BaseEstimator` as well.\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.base import BaseEstimator, ClassifierMixin\r\n\r\nclass MyEstimator(ClassifierMixin, BaseEstimator):\r\n    def __init__(self, *, param=1):\r\n        self.param = param\r\n\r\n    def fit(self, X, y=None):\r\n        self.is_fitted_ = True\r\n        return self\r\n\r\n    def predict(self, X):\r\n        return np.full(shape=X.shape[0], fill_value=self.param)\r\n```\r\n\r\nThe code snippet above runs fine with your `.fit()` and `.predict()` calls. `sklearn` also provides `sklearn.utils.estimator_checks.check_estimator` which you can validate your custom estimator as well. I myself find it very useful for production-grade implementation.\n@gunsodo  \r\nYes we are not exactly following the documentation here. \r\nWe will change that on our side in the future for more robust code.\r\n\r\nThanks for the hint to the check_estimator we'll look into it.\r\n\r\nBut i also think the ClassifierMixIn shouldn't use a method which it can't execute.\r\n\nI would still consider this breakage as a regression. We made effort to raise a proper deprecation warning such that user that defined compatible estimators have one version to change their code.\r\n\r\nWhile we strongly advocate for using `BaseEstimator` and would request it in 1.7, I think that the `Mixin` should indeed not break but instead raise the same deprecation warnings than when one inherit from `BaseEstimator`.\r\n\r\nWe should probably have a try/except around the `super` call and if failing, get the `default_tags` and populate it.\n@adrinjalali do you think that it is a reasonable way forward?\nI'd rather make `ClassifierMixin` inherit from `BaseEstimator` instead (it's not a mixin per say then anymore though).\r\n\r\nWe certainly shouldn't \"fix\" the issue as proposed in #30480 since it breaks code if inheritance order is wrong.\r\n\r\nThe code written here in the OP is simply \"not supported\", however, it's the kinda thing that was supported before, albeit dysfunctional and wrong.\r\n\r\nI'd suggest two things:\r\n\r\n- improve the inheritance order check test, to make sure if there's any of the mixins present, `BaseEstimator` is also present, and call the test before trying to get tags\r\n- The issue here is coming from `get_tags` called in `check_is_fitted`, which was a fix in this release too. We can make `get_tags` catch that particular exception, raise a deprecation warning, and return default tags, and the warning says that the code \"runs\", but it's probably wrong.", "created_at": "2024-12-20T18:25:08Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30513, "instance_id": "scikit-learn__scikit-learn-30513", "issue_numbers": ["30503"], "base_commit": "0d46062ebd1ffdae011e345492860a7e29c0eba3", "patch": "diff --git a/doc/modules/array_api.rst b/doc/modules/array_api.rst\nindex 82eb64dec08c6..82d77f60afc9a 100644\n--- a/doc/modules/array_api.rst\n+++ b/doc/modules/array_api.rst\n@@ -9,7 +9,18 @@ Array API support (experimental)\n The `Array API <https://data-apis.org/array-api/latest/>`_ specification defines\n a standard API for all array manipulation libraries with a NumPy-like API.\n Scikit-learn's Array API support requires\n-`array-api-compat <https://github.com/data-apis/array-api-compat>`__ to be installed.\n+`array-api-compat <https://github.com/data-apis/array-api-compat>`__ to be installed,\n+and the environment variable `SCIPY_ARRAY_API` must be set to `1` before importing\n+`scipy` and `scikit-learn`:\n+\n+.. prompt:: bash $\n+\n+   export SCIPY_ARRAY_API=1\n+\n+Please note that this environment variable is intended for temporary use.\n+For more details, refer to SciPy's `Array API documentation\n+<https://docs.scipy.org/doc/scipy/dev/api-dev/array_api.html#using-array-api-standard-support>`_.\n+\n \n Some scikit-learn estimators that primarily rely on NumPy (as opposed to using\n Cython) to implement the algorithmic logic of their `fit`, `predict` or\n@@ -24,6 +35,12 @@ explicitly as explained in the following.\n     Currently, only `array-api-strict`, `cupy`, and `PyTorch` are known to work\n     with scikit-learn's estimators.\n \n+The following video provides an overview of the standard's design principles\n+and how it facilitates interoperability between array libraries:\n+\n+- `Scikit-learn on GPUs with Array API <https://www.youtube.com/watch?v=c_s8tr1AizA>`_\n+  by :user:`Thomas Fan <thomasjpfan>` at PyData NYC 2023.\n+\n Example usage\n =============\n \n", "test_patch": "", "problem_statement": "Mention setting env variable SCIPY_ARRAY_API=1 in Array API support doc\n### Describe the issue linked to the documentation\r\n\r\nhttps://scikit-learn.org/dev/modules/array_api.html#array-api-support-experimental does not mention `SCIPY_ARRAY_API=1`\r\n\r\n\r\n### Suggest a potential alternative/fix\r\n\r\nMaybe it should mention setting `SCIPY_ARRAY_API=1`.\r\n\r\nI guess you get an error message about it but mentioning it in the doc similarly to installing array-api-compat would make sense.\n", "hints_text": "I\u2019d like to take on this issue if it\u2019s available.\r\n\r\nWould you also like me to add a similar note to the [Parallelism, resource management, and configuration section](https://scikit-learn.org/stable/computing/parallelism.html) of the User Guide? Based on my experience, I need to set the environment variables `SKLEARN_WARNINGS_AS_ERRORS=1` and `SCIPY_ARRAY_API=1` together whenever I\u2019m working on the Array API Project.\nSure go for it!\r\n\r\nFor now I would only add it to the https://scikit-learn.org/dev/modules/array_api.html#array-api-support-experimental. My main focus is end-users.\r\n\r\nPeople that work on Array API support get the error message about the env variable if they don't set it and kind of know what to do I would say. It would also feel a bit weird to add it to the list of the `SKLEARN_` environment variable since after all this is not a `SKLEARN_` environment variable but a `SCIPY_` one ...", "created_at": "2024-12-20T09:03:45Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30497, "instance_id": "scikit-learn__scikit-learn-30497", "issue_numbers": ["30390"], "base_commit": "a0872662e9d444cc8d35f7100a3bfc0539bbf2ea", "patch": "diff --git a/.github/workflows/cuda-ci.yml b/.github/workflows/cuda-ci.yml\nindex ad00e0717a1bf..59c86f15926b1 100644\n--- a/.github/workflows/cuda-ci.yml\n+++ b/.github/workflows/cuda-ci.yml\n@@ -71,6 +71,6 @@ jobs:\n           conda activate sklearn\n           python -c \"import sklearn; sklearn.show_versions()\"\n \n-          SCIPY_ARRAY_API=1 pytest --pyargs sklearn -k 'array_api'\n+          SCIPY_ARRAY_API=1 pytest --pyargs sklearn -k 'array_api' -v\n         # Run in /home/runner to not load sklearn from the checkout repo\n         working-directory: /home/runner\ndiff --git a/build_tools/update_environments_and_lock_files.py b/build_tools/update_environments_and_lock_files.py\nindex 1c9869cc6be0a..829b35ff204ae 100644\n--- a/build_tools/update_environments_and_lock_files.py\n+++ b/build_tools/update_environments_and_lock_files.py\n@@ -100,9 +100,7 @@ def remove_from(alist, to_remove):\n         \"conda_dependencies\": common_dependencies\n         + [\n             \"ccache\",\n-            # Make sure pytorch comes from the pytorch channel and not conda-forge\n-            \"pytorch::pytorch\",\n-            \"pytorch-cuda\",\n+            \"pytorch-gpu\",\n             \"polars\",\n             \"pyarrow\",\n             \"cupy\",\n", "test_patch": "diff --git a/build_tools/github/pylatest_conda_forge_cuda_array-api_linux-64_conda.lock b/build_tools/github/pylatest_conda_forge_cuda_array-api_linux-64_conda.lock\nindex bdebc0d648176..c1d1995430d7b 100644\n--- a/build_tools/github/pylatest_conda_forge_cuda_array-api_linux-64_conda.lock\n+++ b/build_tools/github/pylatest_conda_forge_cuda_array-api_linux-64_conda.lock\n@@ -1,41 +1,39 @@\n # Generated by conda-lock.\n # platform: linux-64\n-# input_hash: 7044e24fc9243a244c265e4b8c44e1304a8f55cd0cfa2d036ead6f92921d624e\n+# input_hash: ad3ced8bfb037ba949d6129ec446e3900b4e9a23f87df881b5804d13539972c9\n @EXPLICIT\n https://conda.anaconda.org/conda-forge/linux-64/_libgcc_mutex-0.1-conda_forge.tar.bz2#d7c89558ba9fa0495403155b64376d81\n https://conda.anaconda.org/conda-forge/linux-64/ca-certificates-2024.12.14-hbcca054_0.conda#720523eb0d6a9b0f6120c16b2aa4e7de\n-https://conda.anaconda.org/conda-forge/noarch/cuda-version-12.4-h3060b56_3.conda#c9a3fe8b957176e1a8452c6f3431b0d8\n+https://conda.anaconda.org/conda-forge/noarch/cuda-version-11.8-h70ddcb2_3.conda#670f0e1593b8c1d84f57ad5fe5256799\n https://conda.anaconda.org/conda-forge/noarch/font-ttf-dejavu-sans-mono-2.37-hab24e00_0.tar.bz2#0c96522c6bdaed4b1566d11387caaf45\n https://conda.anaconda.org/conda-forge/noarch/font-ttf-inconsolata-3.000-h77eed37_0.tar.bz2#34893075a5c9e55cdafac56607368fc6\n https://conda.anaconda.org/conda-forge/noarch/font-ttf-source-code-pro-2.038-h77eed37_0.tar.bz2#4d59c254e01d9cde7957100457e2d5fb\n https://conda.anaconda.org/conda-forge/noarch/font-ttf-ubuntu-0.83-h77eed37_3.conda#49023d73832ef61042f6a237cb2687e7\n-https://conda.anaconda.org/conda-forge/linux-64/mkl-include-2022.1.0-h84fe81f_915.tar.bz2#2dcd1acca05c11410d4494d7fc7dfa2a\n+https://conda.anaconda.org/conda-forge/noarch/kernel-headers_linux-64-3.10.0-he073ed8_18.conda#ad8527bf134a90e1c9ed35fa0b64318c\n https://conda.anaconda.org/conda-forge/linux-64/python_abi-3.12-5_cp312.conda#0424ae29b104430108f5218a66db7260\n-https://conda.anaconda.org/pytorch/noarch/pytorch-mutex-1.0-cuda.tar.bz2#a948316e36fb5b11223b3fcfa93f8358\n https://conda.anaconda.org/conda-forge/noarch/tzdata-2024b-hc8b5060_0.conda#8ac3367aafb1cc0a068483c580af8015\n-https://conda.anaconda.org/conda-forge/noarch/cuda-cccl_linux-64-12.4.127-ha770c72_2.conda#357fbcd43f1296b02d6738a2295abc24\n-https://conda.anaconda.org/conda-forge/noarch/cuda-cudart-static_linux-64-12.4.127-h85509e4_2.conda#0b0522d8685968f25370d5c36bb9fba3\n-https://conda.anaconda.org/conda-forge/noarch/cuda-cudart_linux-64-12.4.127-h85509e4_2.conda#329163110a96514802e9e64d971edf43\n https://conda.anaconda.org/conda-forge/noarch/fonts-conda-forge-1-0.tar.bz2#f766549260d6815b0c52253f1fb1bb29\n https://conda.anaconda.org/conda-forge/linux-64/ld_impl_linux-64-2.43-h712a8e2_2.conda#048b02e3962f066da18efe3a21b77672\n https://conda.anaconda.org/conda-forge/linux-64/libglvnd-1.7.0-ha4b6fd6_2.conda#434ca7e50e40f4918ab701e3facd59a0\n-https://conda.anaconda.org/conda-forge/noarch/cuda-cudart-dev_linux-64-12.4.127-h85509e4_2.conda#12039deb2a3f103f5756831702bf29fc\n+https://conda.anaconda.org/conda-forge/linux-64/llvm-openmp-19.1.5-h024ca30_0.conda#dc90d15c25a57f641f0b84c271e4761e\n+https://conda.anaconda.org/conda-forge/noarch/sysroot_linux-64-2.17-h4a8ded7_18.conda#0ea96f90a10838f58412aa84fdd9df09\n+https://conda.anaconda.org/conda-forge/linux-64/_openmp_mutex-4.5-2_kmp_llvm.tar.bz2#562b26ba2e19059551a811e72ab7f793\n https://conda.anaconda.org/conda-forge/noarch/fonts-conda-ecosystem-1-0.tar.bz2#fee5683a3f04bd15cbd8318b096a27ab\n https://conda.anaconda.org/conda-forge/linux-64/libegl-1.7.0-ha4b6fd6_2.conda#c151d5eb730e9b7480e6d48c0fc44048\n https://conda.anaconda.org/conda-forge/linux-64/libopengl-1.7.0-ha4b6fd6_2.conda#7df50d44d4a14d6c31a2c54f2cd92157\n-https://conda.anaconda.org/conda-forge/linux-64/_openmp_mutex-4.5-2_kmp_llvm.tar.bz2#562b26ba2e19059551a811e72ab7f793\n https://conda.anaconda.org/conda-forge/linux-64/libgcc-14.2.0-h77fa898_1.conda#3cb76c3f10d3bc7f1105b2fc9db984df\n https://conda.anaconda.org/conda-forge/linux-64/alsa-lib-1.2.13-hb9d3cd8_0.conda#ae1370588aa6a5157c34c73e9bbb36a0\n https://conda.anaconda.org/conda-forge/linux-64/aws-c-common-0.10.6-hb9d3cd8_0.conda#d7d4680337a14001b0e043e96529409b\n https://conda.anaconda.org/conda-forge/linux-64/c-ares-1.34.4-hb9d3cd8_0.conda#e2775acf57efd5af15b8e3d1d74d72d3\n https://conda.anaconda.org/conda-forge/linux-64/libbrotlicommon-1.1.0-hb9d3cd8_2.conda#41b599ed2b02abcfdd84302bff174b23\n-https://conda.anaconda.org/conda-forge/linux-64/libdeflate-1.22-hb9d3cd8_0.conda#b422943d5d772b7cc858b36ad2a92db5\n+https://conda.anaconda.org/conda-forge/linux-64/libdeflate-1.23-h4ddbbb0_0.conda#8dfae1d2e74767e9ce36d5fa0d8605db\n https://conda.anaconda.org/conda-forge/linux-64/libexpat-2.6.4-h5888daf_0.conda#db833e03127376d461e1e13e76f09b6c\n https://conda.anaconda.org/conda-forge/linux-64/libgcc-ng-14.2.0-h69a702a_1.conda#e39480b9ca41323497b05492a63bc35b\n https://conda.anaconda.org/conda-forge/linux-64/libgfortran5-14.2.0-hd5240d6_1.conda#9822b874ea29af082e5d36098d25427d\n https://conda.anaconda.org/conda-forge/linux-64/liblzma-5.6.3-hb9d3cd8_1.conda#2ecf2f1c7e4e21fcfe6423a51a992d84\n https://conda.anaconda.org/conda-forge/linux-64/libstdcxx-14.2.0-hc0a3c3a_1.conda#234a5554c53625688d51062645337328\n https://conda.anaconda.org/conda-forge/linux-64/libutf8proc-2.9.0-hb9d3cd8_1.conda#1e936bd23d737aac62a18e9a1e7f8b18\n+https://conda.anaconda.org/conda-forge/linux-64/libuv-1.49.2-hb9d3cd8_0.conda#070e3c9ddab77e38799d5c30b109c633\n https://conda.anaconda.org/conda-forge/linux-64/libzlib-1.3.1-hb9d3cd8_2.conda#edb0dca6bc32e4f4789199455a1dbeb8\n https://conda.anaconda.org/conda-forge/linux-64/openssl-3.4.0-hb9d3cd8_0.conda#23cc74f77eb99315c0360ec3533147a9\n https://conda.anaconda.org/conda-forge/linux-64/pthread-stubs-0.4-hb9d3cd8_1002.conda#b3c17d95b5a10c6e64a21fa17573e70e\n@@ -70,22 +68,17 @@ https://conda.anaconda.org/conda-forge/linux-64/libuuid-2.38.1-h0b41bf4_0.conda#\n https://conda.anaconda.org/conda-forge/linux-64/libwebp-base-1.4.0-hd590300_0.conda#b26e8aa824079e1be0294e7152ca4559\n https://conda.anaconda.org/conda-forge/linux-64/libxcb-1.17.0-h8a09558_0.conda#92ed62436b625154323d40d5f2f11dd7\n https://conda.anaconda.org/conda-forge/linux-64/libxcrypt-4.4.36-hd590300_1.conda#5aa797f8787fe7a17d1b0821485b5adc\n-https://conda.anaconda.org/conda-forge/linux-64/llvm-openmp-15.0.7-h0cdce71_0.conda#589c9a3575a050b583241c3d688ad9aa\n https://conda.anaconda.org/conda-forge/linux-64/lz4-c-1.10.0-h5888daf_1.conda#9de5350a85c4a20c685259b889aa6393\n https://conda.anaconda.org/conda-forge/linux-64/mysql-common-9.0.1-h266115a_3.conda#9411c61ff1070b5e065b32840c39faa5\n https://conda.anaconda.org/conda-forge/linux-64/ncurses-6.5-he02047a_1.conda#70caf8bb6cf39a0b6b7efc885f51c0fe\n-https://conda.anaconda.org/conda-forge/linux-64/opencl-headers-2024.10.24-h5888daf_0.conda#3ba02cce423fdac1a8582bd6bb189359\n https://conda.anaconda.org/conda-forge/linux-64/pixman-0.44.2-h29eaf8c_0.conda#5e2a7acfa2c24188af39e7944e1b3604\n https://conda.anaconda.org/conda-forge/linux-64/s2n-1.5.9-h0fd0ee4_0.conda#f472432f3753c5ca763d2497e2ea30bf\n+https://conda.anaconda.org/conda-forge/linux-64/sleef-3.7-h1b44611_2.conda#4792f3259c6fdc0b730563a85b211dc0\n https://conda.anaconda.org/conda-forge/linux-64/snappy-1.2.1-h8bd8927_1.conda#3b3e64af585eadfb52bb90b553db5edf\n https://conda.anaconda.org/conda-forge/linux-64/tk-8.6.13-noxft_h4845f30_101.conda#d453b98d9c83e71da0741bb0ff4d76bc\n-https://conda.anaconda.org/conda-forge/linux-64/yaml-0.2.5-h7f98852_2.tar.bz2#4cb3ad778ec2d5a7acbdf254eb1c42ae\n https://conda.anaconda.org/conda-forge/linux-64/aws-c-io-0.15.3-hbf5b6a4_4.conda#ad3a6713063c18b9232c48e89ada03ac\n https://conda.anaconda.org/conda-forge/linux-64/brotli-bin-1.1.0-hb9d3cd8_2.conda#c63b5e52939e795ba8d26e35d767a843\n-https://conda.anaconda.org/conda-forge/linux-64/cuda-cudart-12.4.127-he02047a_2.conda#a748faa52331983fc3adcc3b116fe0e4\n-https://conda.anaconda.org/conda-forge/linux-64/cuda-cupti-12.4.127-he02047a_2.conda#46422ef1b1161fb180027e50c598ecd0\n-https://conda.anaconda.org/conda-forge/linux-64/cuda-nvrtc-12.4.127-he02047a_2.conda#80baf6262f4a1a0dde42d85aaa393402\n-https://conda.anaconda.org/conda-forge/linux-64/cuda-nvtx-12.4.127-he02047a_2.conda#656a004b6e44f50ce71c65cab0d429b4\n+https://conda.anaconda.org/conda-forge/linux-64/cudatoolkit-11.8.0-h4ba93d1_13.conda#eb43f5f1f16e2fad2eba22219c3e499b\n https://conda.anaconda.org/conda-forge/linux-64/double-conversion-3.3.0-h59595ed_0.conda#c2f83a5ddadadcdb08fe05863295ee97\n https://conda.anaconda.org/conda-forge/linux-64/freetype-2.12.1-h267a509_2.conda#9ae35c3d96db2c94ce0cef86efdfa2cb\n https://conda.anaconda.org/conda-forge/linux-64/glog-0.7.1-hbabe93e_0.conda#ff862eebdfeb2fd048ae9dc92510baca\n@@ -94,22 +87,16 @@ https://conda.anaconda.org/conda-forge/linux-64/graphite2-1.3.13-h59595ed_1003.c\n https://conda.anaconda.org/conda-forge/linux-64/icu-75.1-he02047a_0.conda#8b189310083baabfb622af68fd9d3ae3\n https://conda.anaconda.org/conda-forge/linux-64/lerc-4.0.0-h27087fc_0.tar.bz2#76bbff344f0134279f225174e9064c8f\n https://conda.anaconda.org/conda-forge/linux-64/libcrc32c-1.1.2-h9c3ff4c_0.tar.bz2#c965a5aa0d5c1c37ffc62dff36e28400\n-https://conda.anaconda.org/conda-forge/linux-64/libcufft-11.2.1.3-he02047a_2.conda#d2641a67c207946ef96f1328c4a8e8ed\n-https://conda.anaconda.org/conda-forge/linux-64/libcufile-1.9.1.3-he02047a_2.conda#a051267bcb1912467c81d802a7d3465e\n-https://conda.anaconda.org/conda-forge/linux-64/libcurand-10.3.5.147-he02047a_2.conda#9c4886d513fd477df88d411cd274c202\n https://conda.anaconda.org/conda-forge/linux-64/libdrm-2.4.124-hb9d3cd8_0.conda#8bc89311041d7fcb510238cf0848ccae\n https://conda.anaconda.org/conda-forge/linux-64/libedit-3.1.20191231-he28a2e2_2.tar.bz2#4d331e44109e3f0e19b4cb8f9b82f3e1\n https://conda.anaconda.org/conda-forge/linux-64/libgfortran-ng-14.2.0-h69a702a_1.conda#0a7f4cd238267c88e5d69f7826a407eb\n https://conda.anaconda.org/conda-forge/linux-64/libnghttp2-1.64.0-h161d5f1_0.conda#19e57602824042dfd0446292ef90488b\n-https://conda.anaconda.org/conda-forge/linux-64/libnpp-12.2.5.30-he02047a_2.conda#a96a1edd18bee676cf2dcca251d3d6a4\n-https://conda.anaconda.org/conda-forge/linux-64/libnvfatbin-12.4.127-he02047a_2.conda#d746b76642b4ac6e40f1219405672beb\n-https://conda.anaconda.org/conda-forge/linux-64/libnvjitlink-12.4.127-he02047a_2.conda#303845d6c48bf4185dc4138634650468\n-https://conda.anaconda.org/conda-forge/linux-64/libnvjpeg-12.3.1.117-he02047a_2.conda#8f3ed0e41a4b505de40b4f96f4bfb0fa\n+https://conda.anaconda.org/conda-forge/linux-64/libopenblas-0.3.28-pthreads_h94d23a6_1.conda#62857b389e42b36b686331bec0922050\n https://conda.anaconda.org/conda-forge/linux-64/libprotobuf-5.28.2-h5b01275_0.conda#ab0bff36363bec94720275a681af8b83\n https://conda.anaconda.org/conda-forge/linux-64/libre2-11-2024.07.02-hbbce691_1.conda#2124de47357b7a516c0a3efd8f88c143\n https://conda.anaconda.org/conda-forge/linux-64/libthrift-0.21.0-h0e7cc3e_0.conda#dcb95c0a98ba9ff737f7ae482aef7833\n+https://conda.anaconda.org/conda-forge/linux-64/nccl-2.23.4.1-h03a54cd_3.conda#5ea398a88c7271b2e3ec56cd33da424f\n https://conda.anaconda.org/conda-forge/linux-64/ninja-1.12.1-h297d8ca_0.conda#3aa1c7e292afeff25a0091ddd7c69b72\n-https://conda.anaconda.org/conda-forge/linux-64/ocl-icd-2.3.2-hb9d3cd8_2.conda#2e8d2b469559d6b2cb6fd4b34f9c8d7f\n https://conda.anaconda.org/conda-forge/linux-64/pcre2-10.44-hba22ea6_2.conda#df359c09c41cd186fffb93a2d87aa6f5\n https://conda.anaconda.org/conda-forge/linux-64/qhull-2020.2-h434a139_5.conda#353823361b1d27eb3960efb076dfcaf6\n https://conda.anaconda.org/conda-forge/linux-64/readline-8.2-h8228510_1.conda#47d31b792659ce70f470b5c82fdfb7a4\n@@ -124,18 +111,18 @@ https://conda.anaconda.org/conda-forge/linux-64/zstd-1.5.6-ha6fb4c9_0.conda#4d05\n https://conda.anaconda.org/conda-forge/linux-64/aws-c-event-stream-0.5.0-h7959bf6_11.conda#9b3fb60fe57925a92f399bc3fc42eccf\n https://conda.anaconda.org/conda-forge/linux-64/aws-c-http-0.9.2-hefd7a92_4.conda#5ce4df662d32d3123ea8da15571b6f51\n https://conda.anaconda.org/conda-forge/linux-64/brotli-1.1.0-hb9d3cd8_2.conda#98514fe74548d768907ce7a13f680e8f\n-https://conda.anaconda.org/conda-forge/linux-64/cuda-opencl-12.4.127-he02047a_1.conda#1e98deda07c14d26c80d124cf0eb011a\n+https://conda.anaconda.org/conda-forge/linux-64/cudnn-9.3.0.75-h50b6be5_1.conda#660be3f87f4cd47853bedaebce9ec76e\n https://conda.anaconda.org/conda-forge/linux-64/fontconfig-2.15.0-h7e30c49_1.conda#8f5b0b297b59e1ac160ad4beec99dbee\n https://conda.anaconda.org/conda-forge/linux-64/krb5-1.21.3-h659f571_0.conda#3f43953b7d3fb3aaa1d0d0723d91e368\n-https://conda.anaconda.org/conda-forge/linux-64/libcublas-12.4.5.8-he02047a_2.conda#d446adae085aa1ff37c44b69988a6f06\n-https://conda.anaconda.org/conda-forge/linux-64/libcusparse-12.3.1.170-he02047a_2.conda#1c4c7ff54dc5b947f2ab8f5ff8a28dae\n+https://conda.anaconda.org/conda-forge/linux-64/libblas-3.9.0-25_linux64_openblas.conda#8ea26d42ca88ec5258802715fe1ee10b\n https://conda.anaconda.org/conda-forge/linux-64/libglib-2.82.2-h2ff4ddf_0.conda#13e8e54035ddd2b91875ba399f0f7c04\n https://conda.anaconda.org/conda-forge/linux-64/libglx-1.7.0-ha4b6fd6_2.conda#c8013e438185f33b13814c5c488acd5c\n https://conda.anaconda.org/conda-forge/linux-64/libhiredis-1.0.2-h2cc385e_0.tar.bz2#b34907d3a81a3cd8095ee83d174c074a\n-https://conda.anaconda.org/conda-forge/linux-64/libtiff-4.7.0-hc4654cb_2.conda#be54fb40ea32e8fe9dbaa94d4528b57e\n+https://conda.anaconda.org/conda-forge/linux-64/libtiff-4.7.0-hd9ff511_3.conda#0ea6510969e1296cc19966fad481f6de\n https://conda.anaconda.org/conda-forge/linux-64/libxml2-2.13.5-h8d12d68_1.conda#1a21e49e190d1ffe58531a81b6e400e1\n https://conda.anaconda.org/conda-forge/linux-64/mpfr-4.2.1-h90cbb55_3.conda#2eeb50cab6652538eee8fc0bc3340c81\n https://conda.anaconda.org/conda-forge/linux-64/mysql-libs-9.0.1-he0572af_3.conda#dd9da69dd4c2bf798c0b8bd4786cafb5\n+https://conda.anaconda.org/conda-forge/linux-64/openblas-0.3.28-pthreads_h6ec200e_1.conda#8fe5d50db07e92519cc639cb0aef9b1b\n https://conda.anaconda.org/conda-forge/linux-64/orc-2.0.3-h97ab989_1.conda#2f46eae652623114e112df13fae311cf\n https://conda.anaconda.org/conda-forge/linux-64/python-3.12.8-h9e4cc4f_1_cpython.conda#7fd2fd79436d9b473812f14e86746844\n https://conda.anaconda.org/conda-forge/linux-64/re2-2024.07.02-h77b4e00_1.conda#01093ff37c1b5e6bf9f17c0116747d11\n@@ -149,7 +136,6 @@ https://conda.anaconda.org/conda-forge/linux-64/aws-c-auth-0.8.0-hb921021_15.con\n https://conda.anaconda.org/conda-forge/linux-64/aws-c-mqtt-0.11.0-h11f4f37_12.conda#96c3e0221fa2da97619ee82faa341a73\n https://conda.anaconda.org/conda-forge/linux-64/cairo-1.18.2-h3394656_1.conda#b34c2833a1f56db610aeb27f206d800d\n https://conda.anaconda.org/conda-forge/linux-64/ccache-4.10.1-h065aff2_0.conda#d6b48c138e0c8170a6fe9c136e063540\n-https://conda.anaconda.org/conda-forge/noarch/certifi-2024.8.30-pyhd8ed1ab_0.conda#12f7d00853807b0531775e9be891cb11\n https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.6-pyhd8ed1ab_1.conda#962b9857ee8e7018c22f2776ffa0b2d7\n https://conda.anaconda.org/conda-forge/noarch/cpython-3.12.8-py312hd8ed1ab_1.conda#caa04d37126e82822468d6bdf50f5ebd\n https://conda.anaconda.org/conda-forge/noarch/cycler-0.12.1-pyhd8ed1ab_1.conda#44600c4667a319d67dbe0681fc0bc833\n@@ -160,15 +146,17 @@ https://conda.anaconda.org/conda-forge/noarch/exceptiongroup-1.2.2-pyhd8ed1ab_1.\n https://conda.anaconda.org/conda-forge/noarch/execnet-2.1.1-pyhd8ed1ab_1.conda#a71efeae2c160f6789900ba2631a2c90\n https://conda.anaconda.org/conda-forge/linux-64/fastrlock-0.8.2-py312h30efb56_2.conda#7065ec5a4909f925e305b77e505b0aec\n https://conda.anaconda.org/conda-forge/noarch/filelock-3.16.1-pyhd8ed1ab_1.conda#d692e9ba6f92dc51484bf3477e36ce7c\n+https://conda.anaconda.org/conda-forge/noarch/fsspec-2024.10.0-pyhd8ed1ab_1.conda#906fe13095e734cb413b57a49116cdc8\n https://conda.anaconda.org/conda-forge/noarch/iniconfig-2.0.0-pyhd8ed1ab_1.conda#6837f3eff7dcea42ecd714ce1ac2b108\n https://conda.anaconda.org/conda-forge/linux-64/kiwisolver-1.4.7-py312h68727a3_0.conda#444266743652a4f1538145e9362f6d3b\n https://conda.anaconda.org/conda-forge/linux-64/lcms2-2.16-hb7c19ff_0.conda#51bb7010fc86f70eee639b4bb7a894f5\n+https://conda.anaconda.org/conda-forge/linux-64/libcblas-3.9.0-25_linux64_openblas.conda#5dbd1b0fc0d01ec5e0e1fbe667281a11\n https://conda.anaconda.org/conda-forge/linux-64/libcups-2.3.3-h4637d8d_4.conda#d4529f4dff3057982a7617c7ac58fde3\n https://conda.anaconda.org/conda-forge/linux-64/libcurl-8.11.1-h332b0f4_0.conda#2b3e0081006dc21e8bf53a91c83a055c\n-https://conda.anaconda.org/conda-forge/linux-64/libcusolver-11.6.1.9-he02047a_2.conda#9f6877f8936be962f598db5e9b8efc51\n https://conda.anaconda.org/conda-forge/linux-64/libgl-1.7.0-ha4b6fd6_2.conda#928b8be80851f5d8ffb016f9c81dae7a\n https://conda.anaconda.org/conda-forge/linux-64/libgrpc-1.67.1-hc2c308b_0.conda#4606a4647bfe857e3cfe21ca12ac3afb\n https://conda.anaconda.org/conda-forge/linux-64/libhwloc-2.11.2-default_h0d58e46_1001.conda#804ca9e91bcaea0824a341d55b1684f2\n+https://conda.anaconda.org/conda-forge/linux-64/liblapack-3.9.0-25_linux64_openblas.conda#4dc03a53fc69371a6158d0ed37214cd3\n https://conda.anaconda.org/conda-forge/linux-64/libllvm19-19.1.5-ha7bfdaf_0.conda#76f3749eda7b24816aacd55b9f31447a\n https://conda.anaconda.org/conda-forge/linux-64/libxkbcommon-1.7.0-h2c5496b_1.conda#e2eaefa4de2b7237af7c907b8bbc760a\n https://conda.anaconda.org/conda-forge/linux-64/libxslt-1.1.39-h76b75d6_0.conda#e71f31f8cfb0a91439f2086fc8aa0461\n@@ -179,11 +167,11 @@ https://conda.anaconda.org/conda-forge/noarch/munkres-1.1.4-pyh9f0ad1d_0.tar.bz2\n https://conda.anaconda.org/conda-forge/noarch/networkx-3.4.2-pyh267e887_2.conda#fd40bf7f7f4bc4b647dc8512053d9873\n https://conda.anaconda.org/conda-forge/linux-64/openjpeg-2.5.3-h5fbd93e_0.conda#9e5816bc95d285c115a3ebc2f8563564\n https://conda.anaconda.org/conda-forge/noarch/packaging-24.2-pyhd8ed1ab_2.conda#3bfed7e6228ebf2f7b9eaa47f1b4e2aa\n+https://conda.anaconda.org/conda-forge/noarch/pip-24.3.1-pyh145f28c_1.conda#04b95993de18684b24bb742ffe0e90a8\n https://conda.anaconda.org/conda-forge/noarch/pluggy-1.5.0-pyhd8ed1ab_1.conda#e9dcbce5f45f9ee500e728ae58b605b6\n https://conda.anaconda.org/conda-forge/noarch/pyparsing-3.2.0-pyhd8ed1ab_2.conda#4c05a2bcf87bb495512374143b57cf28\n https://conda.anaconda.org/conda-forge/noarch/python-tzdata-2024.2-pyhd8ed1ab_1.conda#c0def296b2f6d2dd7b030c2a7f66bb1f\n https://conda.anaconda.org/conda-forge/noarch/pytz-2024.1-pyhd8ed1ab_0.conda#3eeeeb9e4827ace8c0c1419c85d590ad\n-https://conda.anaconda.org/conda-forge/linux-64/pyyaml-6.0.2-py312h66e93f0_1.conda#549e5930e768548a89c23f595dac5a95\n https://conda.anaconda.org/conda-forge/noarch/setuptools-75.6.0-pyhff2d567_1.conda#fc80f7995e396cbaeabd23cf46c413dc\n https://conda.anaconda.org/conda-forge/noarch/six-1.17.0-pyhd8ed1ab_0.conda#a451d576819089b0d672f18768be0f65\n https://conda.anaconda.org/conda-forge/noarch/threadpoolctl-3.5.0-pyhc1e730c_0.conda#df68d78237980a159bd7149f33c0e8fd\n@@ -192,7 +180,6 @@ https://conda.anaconda.org/conda-forge/noarch/tomli-2.2.1-pyhd8ed1ab_1.conda#ac9\n https://conda.anaconda.org/conda-forge/linux-64/tornado-6.4.2-py312h66e93f0_0.conda#e417822cb989e80a0d2b1b576fdd1657\n https://conda.anaconda.org/conda-forge/noarch/typing_extensions-4.12.2-pyha770c72_1.conda#d17f13df8b65464ca316cbc000a3cb64\n https://conda.anaconda.org/conda-forge/linux-64/unicodedata2-15.1.0-py312h66e93f0_1.conda#588486a61153f94c7c13816f7069e440\n-https://conda.anaconda.org/conda-forge/noarch/wheel-0.45.1-pyhd8ed1ab_1.conda#75cb7132eb58d97896e173ef12ac9986\n https://conda.anaconda.org/conda-forge/linux-64/xcb-util-cursor-0.1.5-hb9d3cd8_0.conda#eb44b3b6deb1cab08d72cb61686fe64c\n https://conda.anaconda.org/conda-forge/linux-64/xorg-libxcomposite-0.4.6-hb9d3cd8_2.conda#d3c295b50f092ab525ffe3c2aa4b7413\n https://conda.anaconda.org/conda-forge/linux-64/xorg-libxcursor-1.2.3-hb9d3cd8_0.conda#2ccd714aa2242315acaf0a67faea780b\n@@ -203,7 +190,6 @@ https://conda.anaconda.org/conda-forge/linux-64/xorg-libxxf86vm-1.1.6-hb9d3cd8_0\n https://conda.anaconda.org/conda-forge/linux-64/aws-c-s3-0.7.7-hf454442_0.conda#947c82025693bebd557f782bb5d6b469\n https://conda.anaconda.org/conda-forge/linux-64/azure-core-cpp-1.14.0-h5cfcd09_0.conda#0a8838771cc2e985cd295e01ae83baf1\n https://conda.anaconda.org/conda-forge/linux-64/coverage-7.6.9-py312h178313f_0.conda#a6a5f52f8260983b0aaeebcebf558a3e\n-https://conda.anaconda.org/conda-forge/linux-64/cuda-libraries-12.4.1-ha770c72_1.conda#6bb3f998485d4344a7539e0b218b3fc1\n https://conda.anaconda.org/conda-forge/linux-64/fonttools-4.55.3-py312h178313f_0.conda#968104bfe69e21fadeb30edd9c3785f9\n https://conda.anaconda.org/conda-forge/linux-64/gmpy2-2.1.5-py312h7201bc8_3.conda#673ef4d6611f5b4ca7b5c1f8c65a38dc\n https://conda.anaconda.org/conda-forge/linux-64/harfbuzz-9.0.0-hda332d3_1.conda#76b32dcf243444aea9c6b804bcfa40b8\n@@ -212,56 +198,52 @@ https://conda.anaconda.org/conda-forge/noarch/joblib-1.4.2-pyhd8ed1ab_1.conda#bf\n https://conda.anaconda.org/conda-forge/linux-64/libclang-cpp19.1-19.1.5-default_hb5137d0_0.conda#ec8649c89988d8a443c252c20f259b72\n https://conda.anaconda.org/conda-forge/linux-64/libclang13-19.1.5-default_h9c6a7e4_0.conda#a3a5997b6b47373f0c1608d8503eb4e6\n https://conda.anaconda.org/conda-forge/linux-64/libgoogle-cloud-2.32.0-h804f50b_0.conda#3d96df4d6b1c88455e05b94ce8a14a53\n-https://conda.anaconda.org/conda-forge/noarch/meson-1.6.0-pyhd8ed1ab_1.conda#59d45dbe1b0a123966266340b579d366\n+https://conda.anaconda.org/conda-forge/linux-64/liblapacke-3.9.0-25_linux64_openblas.conda#8f5ead31b3a168aedd488b8a87736c41\n+https://conda.anaconda.org/conda-forge/linux-64/libmagma-2.8.0-h9ddd185_1.conda#2ed47b19940065845dae91ee58ef7957\n+https://conda.anaconda.org/conda-forge/noarch/meson-1.6.1-pyhd8ed1ab_0.conda#0062fb0a7f5da474705d0ce626de12f4\n+https://conda.anaconda.org/conda-forge/linux-64/numpy-2.2.0-py312h7e784f5_0.conda#c9e9a81299192e77428f40711a4fb00d\n https://conda.anaconda.org/conda-forge/linux-64/openldap-2.6.9-he970967_0.conda#ca2de8bbdc871bce41dbf59e51324165\n https://conda.anaconda.org/conda-forge/linux-64/pillow-11.0.0-py312h7b63e92_0.conda#385f46a4df6f97892503a841121a9acf\n-https://conda.anaconda.org/conda-forge/noarch/pip-24.3.1-pyh8b19718_0.conda#5dd546fe99b44fda83963d15f84263b7\n https://conda.anaconda.org/conda-forge/noarch/pyproject-metadata-0.9.0-pyhd8ed1ab_1.conda#1239146a53a383a84633800294120f17\n https://conda.anaconda.org/conda-forge/noarch/pytest-8.3.4-pyhd8ed1ab_1.conda#799ed216dc6af62520f32aa39bc1c2bb\n https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.9.0.post0-pyhff2d567_1.conda#5ba79d7c71f03c678c8ead841f347d6e\n https://conda.anaconda.org/conda-forge/linux-64/tbb-2021.13.0-hceb3a55_1.conda#ba7726b8df7b9d34ea80e82b097a4893\n https://conda.anaconda.org/conda-forge/linux-64/xorg-libxtst-1.2.5-hb9d3cd8_3.conda#7bbe9a0cc0df0ac5f5a8ad6d6a11af2f\n+https://conda.anaconda.org/conda-forge/noarch/array-api-strict-2.2-pyhd8ed1ab_0.conda#5f8d3e0f6b42318772d0f1cdddfe3025\n https://conda.anaconda.org/conda-forge/linux-64/aws-crt-cpp-0.29.7-hd92328a_7.conda#02b95564257d5c3db9c06beccf711f95\n https://conda.anaconda.org/conda-forge/linux-64/azure-identity-cpp-1.10.0-h113e628_0.conda#73f73f60854f325a55f1d31459f2ab73\n https://conda.anaconda.org/conda-forge/linux-64/azure-storage-common-cpp-12.8.0-h736e048_1.conda#13de36be8de3ae3f05ba127631599213\n-https://conda.anaconda.org/conda-forge/noarch/cuda-runtime-12.4.1-ha804496_0.conda#48829f4ef6005ae8d4867b99168ff2b8\n+https://conda.anaconda.org/conda-forge/linux-64/blas-devel-3.9.0-25_linux64_openblas.conda#02c516384c77f5a7b4d03ed6c0412c57\n+https://conda.anaconda.org/conda-forge/linux-64/contourpy-1.3.1-py312h68727a3_0.conda#f5fbba0394ee45e9a64a73c2a994126a\n+https://conda.anaconda.org/conda-forge/linux-64/cupy-core-13.3.0-py312haa09b14_2.conda#565acd25611fce8f002b9ed10bd07165\n https://conda.anaconda.org/conda-forge/linux-64/libgoogle-cloud-storage-2.32.0-h0121fbd_0.conda#877a5ec0431a5af83bf0cd0522bfe661\n+https://conda.anaconda.org/conda-forge/linux-64/libmagma_sparse-2.8.0-h9ddd185_0.conda#f4eb3cfeaf9d91e72d5b2b8706bf059f\n https://conda.anaconda.org/conda-forge/linux-64/libpq-17.2-h3b95a9b_1.conda#37724d8bae042345a19ca1a25dde786b\n https://conda.anaconda.org/conda-forge/noarch/meson-python-0.17.1-pyh70fd9c4_1.conda#7a02679229c6c2092571b4c025055440\n-https://conda.anaconda.org/conda-forge/linux-64/mkl-2022.1.0-h84fe81f_915.tar.bz2#b9c8f925797a93dbff45e1626b025a6b\n+https://conda.anaconda.org/conda-forge/linux-64/mkl-2024.2.2-ha957f24_16.conda#1459379c79dda834673426504d52b319\n+https://conda.anaconda.org/conda-forge/linux-64/pandas-2.2.3-py312hf9745cd_1.conda#8bce4f6caaf8c5448c7ac86d87e26b4b\n+https://conda.anaconda.org/conda-forge/linux-64/polars-1.17.1-py312hda0fa55_0.conda#7ac74b8f85b43224508108f850617dad\n https://conda.anaconda.org/conda-forge/noarch/pytest-cov-6.0.0-pyhd8ed1ab_1.conda#79963c319d1be62c8fd3e34555816e01\n https://conda.anaconda.org/conda-forge/noarch/pytest-xdist-3.6.1-pyhd8ed1ab_1.conda#59aad4fb37cabc0bacc73cf344612ddd\n+https://conda.anaconda.org/conda-forge/linux-64/scipy-1.14.1-py312h62794b6_2.conda#94688dd449f6c092e5f951780235aca1\n https://conda.anaconda.org/conda-forge/noarch/sympy-1.13.3-pyh2585a3b_104.conda#68085d736d2b2f54498832b65059875d\n https://conda.anaconda.org/conda-forge/linux-64/aws-sdk-cpp-1.11.458-hc430e4a_4.conda#aeefac461bea1f126653c1285cf5af08\n https://conda.anaconda.org/conda-forge/linux-64/azure-storage-blobs-cpp-12.13.0-h3cf044e_1.conda#7eb66060455c7a47d9dcdbfa9f46579b\n-https://conda.anaconda.org/conda-forge/linux-64/libblas-3.9.0-16_linux64_mkl.tar.bz2#85f61af03fd291dae33150ffe89dc09a\n-https://conda.anaconda.org/conda-forge/linux-64/mkl-devel-2022.1.0-ha770c72_916.tar.bz2#69ba49e445f87aea2cba343a71a35ca2\n-https://conda.anaconda.org/pytorch/linux-64/pytorch-cuda-12.4-hc786d27_7.tar.bz2#06635b1bbf5e2fef4a8b9b282500cd7b\n+https://conda.anaconda.org/conda-forge/linux-64/blas-2.125-openblas.conda#0c46b8a31a587738befc587dd8e52558\n+https://conda.anaconda.org/conda-forge/linux-64/cupy-13.3.0-py312h8e83189_2.conda#75f6ffc66a1f05ce4f09e83511c9d852\n+https://conda.anaconda.org/conda-forge/linux-64/libtorch-2.5.1-cuda118_hb34f2e8_303.conda#da799bf557ff6376a1a58f40bddfb293\n+https://conda.anaconda.org/conda-forge/linux-64/matplotlib-base-3.10.0-py312hd3ec401_0.conda#c27a17a8c54c0d35cf83bbc0de8f7f77\n+https://conda.anaconda.org/conda-forge/linux-64/pyamg-5.2.1-py312hc39e661_1.conda#372efc32220f0dfb603e5b31ffaefa23\n https://conda.anaconda.org/conda-forge/linux-64/qt6-main-6.8.1-h9d28a51_0.conda#7e8e17c44e7af62c77de7a0158afc35c\n https://conda.anaconda.org/conda-forge/linux-64/azure-storage-files-datalake-cpp-12.12.0-ha633028_1.conda#7c1980f89dd41b097549782121a73490\n-https://conda.anaconda.org/conda-forge/linux-64/libcblas-3.9.0-16_linux64_mkl.tar.bz2#361bf757b95488de76c4f123805742d3\n-https://conda.anaconda.org/conda-forge/linux-64/liblapack-3.9.0-16_linux64_mkl.tar.bz2#a2f166748917d6d6e4707841ca1f519e\n https://conda.anaconda.org/conda-forge/linux-64/pyside6-6.8.1-py312h91f0f75_0.conda#0b7900a6d6f6c441acad5e9ab51001ab\n+https://conda.anaconda.org/conda-forge/linux-64/pytorch-2.5.1-cuda118_py312h919e71f_303.conda#f2fd2356f07999ac24b84b097bb96749\n https://conda.anaconda.org/conda-forge/linux-64/libarrow-18.1.0-h44a453e_6_cpu.conda#2cf6d608d6e66506f69797d5c6944c35\n-https://conda.anaconda.org/conda-forge/linux-64/liblapacke-3.9.0-16_linux64_mkl.tar.bz2#44ccc4d4dca6a8d57fa17442bc64b5a1\n-https://conda.anaconda.org/conda-forge/linux-64/numpy-2.2.0-py312h7e784f5_0.conda#c9e9a81299192e77428f40711a4fb00d\n-https://conda.anaconda.org/conda-forge/noarch/array-api-strict-2.2-pyhd8ed1ab_0.conda#5f8d3e0f6b42318772d0f1cdddfe3025\n-https://conda.anaconda.org/conda-forge/linux-64/blas-devel-3.9.0-16_linux64_mkl.tar.bz2#3f92c1c9e1c0e183462c5071aa02cae1\n-https://conda.anaconda.org/conda-forge/linux-64/contourpy-1.3.1-py312h68727a3_0.conda#f5fbba0394ee45e9a64a73c2a994126a\n-https://conda.anaconda.org/conda-forge/linux-64/cupy-core-13.3.0-py312h1acd1a8_2.conda#15e9530e87664584a6b409ecdf5c9264\n+https://conda.anaconda.org/conda-forge/linux-64/matplotlib-3.10.0-py312h7900ff3_0.conda#89cde9791e6f6355266e7d4455207a5b\n+https://conda.anaconda.org/conda-forge/linux-64/pytorch-gpu-2.5.1-cuda126hf7c78f0_303.conda#afaf760e55725108ae78ed41198c49bb\n https://conda.anaconda.org/conda-forge/linux-64/libarrow-acero-18.1.0-hcb10f89_6_cpu.conda#143f9288b64759a6427563f058c62f2b\n https://conda.anaconda.org/conda-forge/linux-64/libparquet-18.1.0-h081d1f1_6_cpu.conda#68788df49ce7480187eb6387f15b2b67\n-https://conda.anaconda.org/conda-forge/linux-64/pandas-2.2.3-py312hf9745cd_1.conda#8bce4f6caaf8c5448c7ac86d87e26b4b\n-https://conda.anaconda.org/conda-forge/linux-64/polars-1.17.1-py312hda0fa55_0.conda#7ac74b8f85b43224508108f850617dad\n https://conda.anaconda.org/conda-forge/linux-64/pyarrow-core-18.1.0-py312h01725c0_0_cpu.conda#ee80934a6c280ff8635f8db5dec11e04\n-https://conda.anaconda.org/conda-forge/linux-64/scipy-1.14.1-py312h62794b6_2.conda#94688dd449f6c092e5f951780235aca1\n-https://conda.anaconda.org/conda-forge/linux-64/blas-2.116-mkl.tar.bz2#c196a26abf6b4f132c88828ab7c2231c\n-https://conda.anaconda.org/conda-forge/linux-64/cupy-13.3.0-py312h7d319b9_2.conda#009ef049020fef7d1541183d52fab5a9\n https://conda.anaconda.org/conda-forge/linux-64/libarrow-dataset-18.1.0-hcb10f89_6_cpu.conda#20ca46a6bc714a6ab189d5b3f46e66d8\n-https://conda.anaconda.org/conda-forge/linux-64/matplotlib-base-3.9.4-py312hd3ec401_0.conda#b39b563d1a75c7b9b623e2a2b42d9e6d\n-https://conda.anaconda.org/conda-forge/linux-64/pyamg-5.2.1-py312hc39e661_1.conda#372efc32220f0dfb603e5b31ffaefa23\n https://conda.anaconda.org/conda-forge/linux-64/libarrow-substrait-18.1.0-h3ee7192_6_cpu.conda#aa313b3168caf98d00b3753f5ba27650\n-https://conda.anaconda.org/conda-forge/linux-64/matplotlib-3.9.4-py312h7900ff3_0.conda#2b1df96ad1f394cb0d3e67a930ac19c0\n https://conda.anaconda.org/conda-forge/linux-64/pyarrow-18.1.0-py312h7900ff3_0.conda#ac65b70df28687c6af4270923c020bdd\n-https://conda.anaconda.org/pytorch/linux-64/pytorch-2.5.1-py3.12_cuda12.4_cudnn9.1.0_0.tar.bz2#42164c6ce8e563c20a542686a8b9b964\n-https://conda.anaconda.org/pytorch/linux-64/torchtriton-3.1.0-py312.tar.bz2#bb4b2d07cb6b9b476e78740c08ba69fe\ndiff --git a/build_tools/github/pylatest_conda_forge_cuda_array-api_linux-64_environment.yml b/build_tools/github/pylatest_conda_forge_cuda_array-api_linux-64_environment.yml\nindex e2ffb1429aa1d..130627b9b7f7b 100644\n--- a/build_tools/github/pylatest_conda_forge_cuda_array-api_linux-64_environment.yml\n+++ b/build_tools/github/pylatest_conda_forge_cuda_array-api_linux-64_environment.yml\n@@ -25,8 +25,7 @@ dependencies:\n   - pytest-cov\n   - coverage\n   - ccache\n-  - pytorch::pytorch\n-  - pytorch-cuda\n+  - pytorch-gpu\n   - polars\n   - pyarrow\n   - cupy\n", "problem_statement": "CI Replace pytorch conda channel in CI lock-files\nAfter a quick look, it seems like the only place we are using the pytorch channel is for the CUDA CI, cc @betatim.\r\n\r\nThe easiest thing to try would be to use the conda-forge pytorch-gpu package?\r\n\r\nSee https://github.com/pytorch/pytorch/issues/138506 for more details. the main thing is:\r\n\r\n> 2.5 will be the last release of PyTorch that will be published to the [pytorch](https://anaconda.org/pytorch) channel on Anaconda\r\n\r\nSo for now, it seems like nothing will break, we will keep using the PyTorch `2.5.*` release, which for now is the latest release (until PyTorch 2.6 is released, not sure about the timeline). \n", "hints_text": "> The easiest thing to try would be to use the conda-forge pytorch-gpu package?\r\n\r\nI think so too. Or we could switch to using the wheel?", "created_at": "2024-12-17T07:45:38Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30496, "instance_id": "scikit-learn__scikit-learn-30496", "issue_numbers": ["29027"], "base_commit": "c2ce72380650f5822529b762adaff82149f3c924", "patch": "diff --git a/build_tools/update_environments_and_lock_files.py b/build_tools/update_environments_and_lock_files.py\nindex 312a54dba4dad..80ece8aee74ba 100644\n--- a/build_tools/update_environments_and_lock_files.py\n+++ b/build_tools/update_environments_and_lock_files.py\n@@ -125,6 +125,7 @@ def remove_from(alist, to_remove):\n             \"pyarrow\",\n             \"array-api-compat\",\n             \"array-api-strict\",\n+            \"scipy-doctest\",\n         ],\n         \"package_constraints\": {\n             \"blas\": \"[build=mkl]\",\n@@ -223,6 +224,8 @@ def remove_from(alist, to_remove):\n             + [\"lightgbm\", \"scikit-image\"]\n             # Test array API on CPU without PyTorch\n             + [\"array-api-compat\", \"array-api-strict\"]\n+            # doctests dependencies\n+            + [\"scipy-doctest\"]\n         ),\n     },\n     {\ndiff --git a/setup.cfg b/setup.cfg\nindex 807fb9ef34784..643cfebfe33cc 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -13,7 +13,6 @@ test = pytest\n doctest_optionflags = NORMALIZE_WHITESPACE ELLIPSIS\n testpaths = sklearn\n addopts =\n-    --doctest-modules\n     --disable-pytest-warnings\n     --color=yes\n \n", "test_patch": "diff --git a/build_tools/azure/pylatest_conda_forge_mkl_linux-64_conda.lock b/build_tools/azure/pylatest_conda_forge_mkl_linux-64_conda.lock\nindex f92b3eb1bf335..9f901ff01e119 100644\n--- a/build_tools/azure/pylatest_conda_forge_mkl_linux-64_conda.lock\n+++ b/build_tools/azure/pylatest_conda_forge_mkl_linux-64_conda.lock\n@@ -1,6 +1,6 @@\n # Generated by conda-lock.\n # platform: linux-64\n-# input_hash: 93ee312868bc5df4bdc9b2ef07f938f6a5922dfe2375c4963a7c63d19c5d87f6\n+# input_hash: e84d504f626e0b12ad18dfa7e6c91af55468946b2f96de1abb6ee2ec5b8816b7\n @EXPLICIT\n https://conda.anaconda.org/conda-forge/linux-64/_libgcc_mutex-0.1-conda_forge.tar.bz2#d7c89558ba9fa0495403155b64376d81\n https://conda.anaconda.org/conda-forge/linux-64/ca-certificates-2024.12.14-hbcca054_0.conda#720523eb0d6a9b0f6120c16b2aa4e7de\n@@ -84,7 +84,7 @@ https://conda.anaconda.org/conda-forge/linux-64/icu-75.1-he02047a_0.conda#8b1893\n https://conda.anaconda.org/conda-forge/linux-64/lerc-4.0.0-h27087fc_0.tar.bz2#76bbff344f0134279f225174e9064c8f\n https://conda.anaconda.org/conda-forge/linux-64/libcrc32c-1.1.2-h9c3ff4c_0.tar.bz2#c965a5aa0d5c1c37ffc62dff36e28400\n https://conda.anaconda.org/conda-forge/linux-64/libdrm-2.4.124-hb9d3cd8_0.conda#8bc89311041d7fcb510238cf0848ccae\n-https://conda.anaconda.org/conda-forge/linux-64/libedit-3.1.20191231-he28a2e2_2.tar.bz2#4d331e44109e3f0e19b4cb8f9b82f3e1\n+https://conda.anaconda.org/conda-forge/linux-64/libedit-3.1.20240808-pl5321h7949ede_0.conda#8247f80f3dc464d9322e85007e307fe8\n https://conda.anaconda.org/conda-forge/linux-64/libgfortran-ng-14.2.0-h69a702a_1.conda#0a7f4cd238267c88e5d69f7826a407eb\n https://conda.anaconda.org/conda-forge/linux-64/libnghttp2-1.64.0-h161d5f1_0.conda#19e57602824042dfd0446292ef90488b\n https://conda.anaconda.org/conda-forge/linux-64/libprotobuf-5.28.3-h6128344_1.conda#d8703f1ffe5a06356f06467f1d0b9464\n@@ -115,7 +115,7 @@ https://conda.anaconda.org/conda-forge/linux-64/libxml2-2.13.5-h8d12d68_1.conda#\n https://conda.anaconda.org/conda-forge/linux-64/mpfr-4.2.1-h90cbb55_3.conda#2eeb50cab6652538eee8fc0bc3340c81\n https://conda.anaconda.org/conda-forge/linux-64/mysql-libs-9.0.1-he0572af_4.conda#af19508df9d2e9f6894a9076a0857dc7\n https://conda.anaconda.org/conda-forge/linux-64/orc-2.0.3-h12ee42a_2.conda#4f6f9f3f80354ad185e276c120eac3f0\n-https://conda.anaconda.org/conda-forge/linux-64/python-3.13.1-ha99a958_102_cp313.conda#6e7535f1d1faf524e9210d2689b3149b\n+https://conda.anaconda.org/conda-forge/linux-64/python-3.13.1-ha99a958_103_cp313.conda#899de8f76e198a36bc5a36132a6db887\n https://conda.anaconda.org/conda-forge/linux-64/re2-2024.07.02-h9925aae_2.conda#e84ddf12bde691e8ec894b00ea829ddf\n https://conda.anaconda.org/conda-forge/linux-64/xcb-util-image-0.4.0-hb711507_2.conda#a0901183f08b6c7107aab109733a3c91\n https://conda.anaconda.org/conda-forge/linux-64/xkeyboard-config-2.43-hb9d3cd8_0.conda#f725c7425d6d7c15e31f3b99a88ea02f\n@@ -128,7 +128,7 @@ https://conda.anaconda.org/conda-forge/linux-64/aws-c-mqtt-0.11.0-h11f4f37_12.co\n https://conda.anaconda.org/conda-forge/linux-64/cairo-1.18.2-h3394656_1.conda#b34c2833a1f56db610aeb27f206d800d\n https://conda.anaconda.org/conda-forge/linux-64/ccache-4.10.1-h065aff2_0.conda#d6b48c138e0c8170a6fe9c136e063540\n https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.6-pyhd8ed1ab_1.conda#962b9857ee8e7018c22f2776ffa0b2d7\n-https://conda.anaconda.org/conda-forge/noarch/cpython-3.13.1-py313hd8ed1ab_102.conda#03f9b71509b4a492d7da023bf825ebbd\n+https://conda.anaconda.org/conda-forge/noarch/cpython-3.13.1-py313hd8ed1ab_103.conda#876543b07b69c9933a2f36ad0a4d46ae\n https://conda.anaconda.org/conda-forge/noarch/cycler-0.12.1-pyhd8ed1ab_1.conda#44600c4667a319d67dbe0681fc0bc833\n https://conda.anaconda.org/conda-forge/linux-64/cyrus-sasl-2.1.27-h54b06d7_7.conda#dce22f70b4e5a407ce88f2be046f4ceb\n https://conda.anaconda.org/conda-forge/linux-64/cython-3.0.11-py313hc66aa0d_3.conda#1778443eb12b2da98428fa69152a2a2e\n@@ -202,7 +202,7 @@ https://conda.anaconda.org/conda-forge/noarch/meson-python-0.17.1-pyh70fd9c4_1.c\n https://conda.anaconda.org/conda-forge/linux-64/mkl-2024.2.2-ha957f24_16.conda#1459379c79dda834673426504d52b319\n https://conda.anaconda.org/conda-forge/noarch/pytest-cov-6.0.0-pyhd8ed1ab_1.conda#79963c319d1be62c8fd3e34555816e01\n https://conda.anaconda.org/conda-forge/noarch/pytest-xdist-3.6.1-pyhd8ed1ab_1.conda#59aad4fb37cabc0bacc73cf344612ddd\n-https://conda.anaconda.org/conda-forge/noarch/sympy-1.13.3-pyh2585a3b_104.conda#68085d736d2b2f54498832b65059875d\n+https://conda.anaconda.org/conda-forge/noarch/sympy-1.13.3-pyh2585a3b_105.conda#254cd5083ffa04d96e3173397a3d30f4\n https://conda.anaconda.org/conda-forge/linux-64/aws-sdk-cpp-1.11.458-hc430e4a_4.conda#aeefac461bea1f126653c1285cf5af08\n https://conda.anaconda.org/conda-forge/linux-64/azure-storage-blobs-cpp-12.13.0-h3cf044e_1.conda#7eb66060455c7a47d9dcdbfa9f46579b\n https://conda.anaconda.org/conda-forge/linux-64/libblas-3.9.0-26_linux64_mkl.conda#60463d3ec26e0860bfc7fc1547e005ef\n@@ -226,6 +226,7 @@ https://conda.anaconda.org/conda-forge/linux-64/polars-1.17.1-py313hae41bca_0.co\n https://conda.anaconda.org/conda-forge/linux-64/pyarrow-core-18.1.0-py313he5f92c8_0_cpu.conda#5380e12f4468e891911dbbd4248b521a\n https://conda.anaconda.org/conda-forge/linux-64/pytorch-2.5.1-cpu_mkl_py313_h90df46e_108.conda#f192f56caccbdbdad81e015a64294e92\n https://conda.anaconda.org/conda-forge/linux-64/scipy-1.15.0-py313hc93385a_0.conda#cd05940add8516cad1407b7dac647526\n+https://conda.anaconda.org/conda-forge/noarch/scipy-doctest-1.6-pyhd8ed1ab_0.conda#7e34ac6c22e725453bfbe0dccb190deb\n https://conda.anaconda.org/conda-forge/linux-64/blas-2.126-mkl.conda#4af53f2542f5adbfc2290f084f3a99fa\n https://conda.anaconda.org/conda-forge/linux-64/libarrow-dataset-18.1.0-hcb10f89_7_cpu.conda#0a81eb63d7cd150f598c752e86388d57\n https://conda.anaconda.org/conda-forge/linux-64/matplotlib-base-3.10.0-py313h129903b_0.conda#ab5b84154e1d9e41d4f11aea76d74096\ndiff --git a/build_tools/azure/pylatest_conda_forge_mkl_linux-64_environment.yml b/build_tools/azure/pylatest_conda_forge_mkl_linux-64_environment.yml\nindex 12fbd178dccb5..c8faab9f186ee 100644\n--- a/build_tools/azure/pylatest_conda_forge_mkl_linux-64_environment.yml\n+++ b/build_tools/azure/pylatest_conda_forge_mkl_linux-64_environment.yml\n@@ -29,3 +29,4 @@ dependencies:\n   - pyarrow\n   - array-api-compat\n   - array-api-strict\n+  - scipy-doctest\ndiff --git a/build_tools/azure/pylatest_pip_openblas_pandas_environment.yml b/build_tools/azure/pylatest_pip_openblas_pandas_environment.yml\nindex 177d28555f712..6661911500e99 100644\n--- a/build_tools/azure/pylatest_pip_openblas_pandas_environment.yml\n+++ b/build_tools/azure/pylatest_pip_openblas_pandas_environment.yml\n@@ -29,3 +29,4 @@ dependencies:\n     - scikit-image\n     - array-api-compat\n     - array-api-strict\n+    - scipy-doctest\ndiff --git a/build_tools/azure/pylatest_pip_openblas_pandas_linux-64_conda.lock b/build_tools/azure/pylatest_pip_openblas_pandas_linux-64_conda.lock\nindex 7d47d2f07bd03..90152a81b8294 100644\n--- a/build_tools/azure/pylatest_pip_openblas_pandas_linux-64_conda.lock\n+++ b/build_tools/azure/pylatest_pip_openblas_pandas_linux-64_conda.lock\n@@ -1,6 +1,6 @@\n # Generated by conda-lock.\n # platform: linux-64\n-# input_hash: 38d3951742eb4e3d26c6768f2c329b12d5418fed96f94c97da19b776b04ee767\n+# input_hash: b5f68a126ac0b46294f6375de7dc7f9deb7a0def13ad92aff1cc9a609ec723d2\n @EXPLICIT\n https://repo.anaconda.com/pkgs/main/linux-64/_libgcc_mutex-0.1-main.conda#c3473ff8bdb3d124ed5ff11ec380d6f9\n https://repo.anaconda.com/pkgs/main/linux-64/ca-certificates-2024.11.26-h06a4308_0.conda#cebd61e6520159a1315d679321620f6c\n@@ -86,5 +86,6 @@ https://repo.anaconda.com/pkgs/main/linux-64/pip-24.2-py313h06a4308_0.conda#59f8\n # pip pytest-cov @ https://files.pythonhosted.org/packages/36/3b/48e79f2cd6a61dbbd4807b4ed46cb564b4fd50a76166b1c4ea5c1d9e2371/pytest_cov-6.0.0-py3-none-any.whl#sha256=eee6f1b9e61008bd34975a4d5bab25801eb31898b032dd55addc93e96fcaaa35\n # pip pytest-xdist @ https://files.pythonhosted.org/packages/6d/82/1d96bf03ee4c0fdc3c0cbe61470070e659ca78dc0086fb88b66c185e2449/pytest_xdist-3.6.1-py3-none-any.whl#sha256=9ed4adfb68a016610848639bb7e02c9352d5d9f03d04809919e2dafc3be4cca7\n # pip scikit-image @ https://files.pythonhosted.org/packages/8c/d2/84d658db2abecac5f7225213a69d211d95157e8fa155b4e017903549a922/scikit_image-0.25.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl#sha256=0fe2f05cda852a5f90872054dd3709e9c4e670fc7332aef169867944e1b37431\n+# pip scipy-doctest @ https://files.pythonhosted.org/packages/d8/c3/209584a4d2638f9c0cceaa81fba8e2a07f75461eda8103aac37f8795481e/scipy_doctest-1.5.1-py3-none-any.whl#sha256=2252582053e2c3fca63eaf5eb7456057dbeebbd4f836551360cfdccdede6c6e3\n # pip sphinx @ https://files.pythonhosted.org/packages/26/60/1ddff83a56d33aaf6f10ec8ce84b4c007d9368b21008876fceda7e7381ef/sphinx-8.1.3-py3-none-any.whl#sha256=09719015511837b76bf6e03e42eb7595ac8c2e41eeb9c29c5b755c6b677992a2\n # pip numpydoc @ https://files.pythonhosted.org/packages/6c/45/56d99ba9366476cd8548527667f01869279cedb9e66b28eb4dfb27701679/numpydoc-1.8.0-py3-none-any.whl#sha256=72024c7fd5e17375dec3608a27c03303e8ad00c81292667955c6fea7a3ccf541\ndiff --git a/build_tools/azure/test_docs.sh b/build_tools/azure/test_docs.sh\nindex 48ad2763edb36..f3f824d5806b0 100755\n--- a/build_tools/azure/test_docs.sh\n+++ b/build_tools/azure/test_docs.sh\n@@ -5,6 +5,17 @@ set -ex\n source build_tools/shared.sh\n activate_environment\n \n-# XXX: for some unknown reason python -m pytest fails here in the CI, can't\n-# reproduce locally and not worth spending time on this\n-pytest $(find doc -name '*.rst' | sort)\n+scipy_doctest_installed=$(python -c 'import scipy_doctest' && echo \"True\" || echo \"False\")\n+if [[ \"$scipy_doctest_installed\" == \"True\" ]]; then\n+    doc_rst_files=$(find $PWD/doc -name '*.rst' | sort)\n+    # Changing dir, as we do in build_tools/azure/test_script.sh, avoids an\n+    # error when importing sklearn. Not sure why this happens ... I am going to\n+    # wild guess that it has something to do with the bespoke way we set up\n+    # conda with putting conda in the PATH and source activate, rather than\n+    # source <conda_root>/etc/profile.d/conda.sh + conda activate.\n+    cd $TEST_DIR\n+    # with scipy-doctest, --doctest-modules only runs doctests (in contrary to\n+    # vanilla pytest where it runs doctests on top of normal tests)\n+    python -m pytest --doctest-modules --pyargs sklearn\n+    python -m pytest --doctest-modules $doc_rst_files\n+fi\ndiff --git a/sklearn/conftest.py b/sklearn/conftest.py\nindex 6c91c5340b486..0c7e00a93c6aa 100644\n--- a/sklearn/conftest.py\n+++ b/sklearn/conftest.py\n@@ -37,6 +37,11 @@\n     sp_version,\n )\n \n+try:\n+    from scipy_doctest.conftest import dt_config\n+except ModuleNotFoundError:\n+    dt_config = None\n+\n if parse_version(pytest.__version__) < parse_version(PYTEST_MIN_VERSION):\n     raise ImportError(\n         f\"Your version of pytest is too old. Got version {pytest.__version__}, you\"\n@@ -356,3 +361,8 @@ def print_changed_only_false():\n     set_config(print_changed_only=False)\n     yield\n     set_config(print_changed_only=True)  # reset to default\n+\n+\n+if dt_config is not None:\n+    # Strict mode to differentiate between 3.14 and np.float64(3.14)\n+    dt_config.strict_check = True\n", "problem_statement": "DOC Investigate scipy-doctest for more convenient doctests\nI learned about [scipy-doctest](https://github.com/scipy/scipy_doctest) recent release in the [Scientific Python Discourse announcement](https://discuss.scientific-python.org/t/ann-scipy-doctest-package/1181). Apparently, scipy-doctest has been used internally in numpy and scipy for doctests for some time. In particular it allows floating point comparisons.\r\n\r\nAfter a bit of work from us setting everything up, it would allow to have a few sprint / first good issues.\r\n\r\nThere is quite a few places where we used the doctest ellipsis, the quick and dirty following regexp finds 595 lines:\r\n```\r\ngit grep -P '\\d+\\.\\.\\.' | wc -l\r\n```\r\n\r\nIf you are not sure what I am talking about, this is the `...` for doctest in rst for docstrings e.g. the last line of this snippet:\r\n```py\r\n>>> from sklearn import svm, datasets\r\n>>> from sklearn.model_selection import cross_val_score\r\n>>> X, y = datasets.load_iris(return_X_y=True)\r\n>>> clf = svm.SVC(random_state=0)\r\n>>> cross_val_score(clf, X, y, cv=5, scoring='recall_macro')\r\narray([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])\r\n```\r\n\r\nAn example of a doctest with a spurious failure recently: https://github.com/scikit-learn/scikit-learn/pull/29140#issuecomment-2139904739\r\n\r\nIf you are wondering about the difference to [pytest-doctestplus](https://github.com/scientific-python/pytest-doctestplus) look at [this](https://github.com/scipy/scipy_doctest?tab=readme-ov-file#prior-art-and-related-work). This does seem a bit unfortunate to have `scipy/scipy_doctest` and `scientific-python/pytest-doctestplus` but oh well (full disclosure I did not have time to look into the history) ...\r\n\r\n\n", "hints_text": "If there's still interest in scipy-doctest, feel free to ping me\nThanks! There is still some interest, I did have a quick go at one point but failed to make it work in the amount of time I had :sweat_smile: \r\n\r\nMaybe you can comment on the general approach? In particular, I am guessing that you can still run the previous doctests unchanged with scipy-doctest?\r\n\r\nOtherwise I guess I (or someone else) would need to look at how numpy or scipy is doing it and adapt it for scikit-learn (for example we already have some logic to skip all doctests for numpy<2 or if matplotlib is not installed):\r\n\r\nhttps://github.com/numpy/numpy/blob/8e6914d1d1586248aac518082d632839767eab91/numpy/conftest.py#L153-L232\r\n\r\nhttps://github.com/scipy/scipy/blob/2fde036ec404512d695568a4259ba16f6c9156b5/scipy/conftest.py#L395-L551\r\n\r\nAfter a quick look it looks like the main things are:\r\n- warnings control (probably because warnings are turned into errors)\r\n- managing a list of tests to skip or xfail\r\n- ignore some tests during collection\r\n\r\n\nYes, the goal is to be able to run doctests unmodified and make them whitespace insensitive, be able to use `# may vary` instead of `# doctest: +SKIP`, not worry about numpy 2.2 vs 1.x formatting etc. Basically, just focus on examples being for the reader, instead of wrangling with the tools.\r\n\r\nTo set it up:\r\n\r\n-  in the conftest.py, add an import, https://github.com/scipy/scipy/blob/2fde036ec404512d695568a4259ba16f6c9156b5/scipy/conftest.py#L19, and you've a set of default settings from scipy's decade of usage. \r\nThe rest of conftests in numpy and scipy is further tweaks to these defaults.\r\n\r\n- how you run tests in scikit-learn? If spin, numpy adds a `spin check-docs` command, https://github.com/numpy/numpy/blob/main/.spin/cmds.py#L156\r\n\r\n- the spin command is of course just syntax sugar on top of `$ pytest --doctest-modules --pyargs installed-module` or, for meson dev installs, `$ pytest --doctest-modules path/to/build-install-dir`.\r\n\r\n- one scipy-doctest specific pytest switch, which both scipy and numpy use is `--doctest-collect=api`. This is to only collect docstrings of public objects. \r\n\r\nWhen `scipy-doctest` is pip-installed in the environment, it automatically hooks into `--doctest-modules`.\r\n\r\nThe bulk of scipy/numpy conftest.py is indeed,\r\n\r\n> - warnings control (probably because warnings are turned into errors)\r\n\r\nYes, that. And we just like to filter out irrelevant known warnings in some cases. Those parts are not essential.\r\n\r\n> managing a list of tests to skip or xfail\r\n\r\nIndeed. The list of things to skip is kept in the tool, not in the docstrings. Because this is irrelevant to a reader of the docs, or a function is deprecated (so its import emits a DeprecationWarning) etc.\r\n\r\n> ignore some tests during collection\r\n\r\nYes, service parts of the library. In numpy, it's distutils for example; in scipy's dev.py, there's an additional list. `pytest-extra-ignore` is just syntax sugar on top of `pytest --ignore path/to/ignore`. \r\n\r\nI don't know if scikit-learn has ReST tutorials. If you do and if you want them to be doctested, numpy has `$ spin check-tutorials`.\r\n\r\n------------------------------\r\n\r\nIf you want, we can have a chat in real time.\nAlso, while we're on \"general approach\" topic: the setup in both scipy and numpy is to maintain a clear separation between doctests and actual unit testing. For one, `--doctest-modules` only runs doctests, not regular unit tests.\r\n\nThanks a lot for the details, I need to take a closer look and I'll definitely ping you if I get stuck.", "created_at": "2024-12-17T06:40:56Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30483, "instance_id": "scikit-learn__scikit-learn-30483", "issue_numbers": ["30625"], "base_commit": "72b35a46684c0ecf4182500d3320836607d1f17c", "patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.covariance/30483.fix.rst b/doc/whats_new/upcoming_changes/sklearn.covariance/30483.fix.rst\nnew file mode 100644\nindex 0000000000000..4329c5a2696fd\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.covariance/30483.fix.rst\n@@ -0,0 +1,2 @@\n+- Support for ``n_samples == n_features`` in `sklearn.covariance.MinCovDet` has\n+  been restored.  By :user:`Antony Lee <anntzer>`.\ndiff --git a/sklearn/covariance/_robust_covariance.py b/sklearn/covariance/_robust_covariance.py\nindex 786c4e17b552b..559401f7bbc5b 100644\n--- a/sklearn/covariance/_robust_covariance.py\n+++ b/sklearn/covariance/_robust_covariance.py\n@@ -433,7 +433,7 @@ def fast_mcd(\n \n     # minimum breakdown value\n     if support_fraction is None:\n-        n_support = int(np.ceil(0.5 * (n_samples + n_features + 1)))\n+        n_support = min(int(np.ceil(0.5 * (n_samples + n_features + 1))), n_samples)\n     else:\n         n_support = int(support_fraction * n_samples)\n \n", "test_patch": "diff --git a/sklearn/covariance/tests/test_robust_covariance.py b/sklearn/covariance/tests/test_robust_covariance.py\nindex ebeb2c6e5aa6b..a7bd3996b9e4b 100644\n--- a/sklearn/covariance/tests/test_robust_covariance.py\n+++ b/sklearn/covariance/tests/test_robust_covariance.py\n@@ -34,6 +34,9 @@ def test_mcd(global_random_seed):\n     # 1D data set\n     launch_mcd_on_dataset(500, 1, 100, 0.02, 0.02, 350, global_random_seed)\n \n+    # n_samples == n_features\n+    launch_mcd_on_dataset(20, 20, 0, 0.1, 0.1, 15, global_random_seed)\n+\n \n def test_fast_mcd_on_invalid_input():\n     X = np.arange(100)\n", "problem_statement": "scikit-learn 1.6: Elliptic Envelope Fails with More Features than Samples\n### Describe the bug\n\nWhen using the EllipticEnvelope class in scikit-learn 1.6, the model raises an error when the number of features exceeds the number of samples in the input dataset. This issue occurs even when the data is preprocessed (e.g., scaled with StandardScaler) and is independent of the contamination or support fraction settings.\r\n\r\nThis behavior differs from previous versions of scikit-learn, where the EllipticEnvelope was able to handle cases with more features than samples without raising errors.\n\n### Steps/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nfrom sklearn.covariance import EllipticEnvelope\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\n# Generate data with more features than samples\r\nX = np.random.rand(5, 10)  # 5 samples, 10 features\r\n\r\n# Preprocess the data\r\nscaler = StandardScaler()\r\nX_scaled = scaler.fit_transform(X)\r\n\r\n# Initialize Elliptic Envelope\r\nmodel = EllipticEnvelope(contamination=0.1)\r\n\r\n# Attempt to fit the model\r\ntry:\r\n    model.fit(X_scaled)\r\n    print(\"EllipticEnvelope successfully fitted.\")\r\nexcept Exception as e:\r\n    print(\"Error encountered:\", e)\r\n```\r\n\n\n### Expected Results\n\nResults with scikit-learn version < 1.6\r\n```python\r\nUserWarning: The covariance matrix associated to your dataset is not full rank\r\n  warnings.warn(\r\nEllipticEnvelope successfully fitted.\r\n```\r\n\n\n### Actual Results\n\nResults with scikit-learn version >= 1.6\r\n```python\r\nUserWarning: The covariance matrix associated to your dataset is not full rank\r\n  warnings.warn(\r\nError encountered: kth(=7) out of bounds (5)\r\n```\n\n### Versions\n\n```shell\nVersions\r\n\r\nSystem:\r\n    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\r\n    executable: <redacted>\r\n    machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.0\r\n          pip: 24.3.1\r\n   setuptools: 75.6.0\r\n        numpy: 2.0.2\r\n        scipy: 1.13.1\r\n       Cython: None\r\n       pandas: 2.2.3\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 4\r\n         prefix: vcomp\r\n       filepath: <redacted>\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 4\r\n         prefix: libscipy_openblas\r\n       filepath: <redacted>\r\n        version: 0.3.27\r\nthreading_layer: pthreads\r\n   architecture: Zen\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 4\r\n         prefix: libopenblas\r\n       filepath: <redacted>\r\n        version: 0.3.27\r\nthreading_layer: pthreads\r\n   architecture: Zen\n```\n\n", "hints_text": "", "created_at": "2024-12-14T12:16:15Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30473, "instance_id": "scikit-learn__scikit-learn-30473", "issue_numbers": ["30430"], "base_commit": "a8bef5fd513cf949ebc411cdc0bd0158894e92aa", "patch": "diff --git a/sklearn/feature_selection/_univariate_selection.py b/sklearn/feature_selection/_univariate_selection.py\nindex 996d5423995d2..855ba5ad70f12 100644\n--- a/sklearn/feature_selection/_univariate_selection.py\n+++ b/sklearn/feature_selection/_univariate_selection.py\n@@ -203,9 +203,12 @@ def chi2(X, y):\n \n     This score can be used to select the `n_features` features with the\n     highest values for the test chi-squared statistic from X, which must\n-    contain only **non-negative features** such as booleans or frequencies\n+    contain only **non-negative integer feature values** such as booleans or frequencies\n     (e.g., term counts in document classification), relative to the classes.\n \n+    If some of your features are continuous, you need to bin them, for\n+    example by using :class:`~sklearn.preprocessing.KBinsDiscretizer`.\n+\n     Recall that the chi-square test measures dependence between stochastic\n     variables, so using this function \"weeds out\" the features that are the\n     most likely to be independent of class and therefore irrelevant for\n", "test_patch": "", "problem_statement": "Example of binning of continous variables for chi2\n### Describe the issue linked to the documentation\n\nThe [chi2](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html) doesn't work on continuous variables. This issue has numerous discussions, e.g. [here](https://stats.stackexchange.com/questions/369945/feature-selection-using-chi-squared-for-continuous-features).\r\n\r\nThe Matlab counterpart command, [fscchi2](https://www.mathworks.com/help/stats/fscchi2.html), solves this issue by automatically binning data. I believe that the example of chi2 feature selection with pre-binning may be beneficial. \n\n### Suggest a potential alternative/fix\n\n_No response_\n", "hints_text": "Hello ! \r\nI would like to work on this issue.\r\n\r\nI am wondering whether it would be better to add an \"automatic_binning\" parameter to the chi2 function or to clarify the documentation by explicitly stating that continuous data must be pre-binned before using chi2.\r\n\r\nIn the second case, it might also be helpful to include an example demonstrating how to bin continuous data before applying chi2. What do you think would be the preferred approach?\nIt is great to know that this issue is relevant.\r\n\r\nAs an immediate solution, some basic binning example is recommended.\r\n\r\nIn the long run, it is recommended that some binning technique has to be implemented. It is important to note, that the multivariate binning is non-trivial. For example, there are different binning strategies, e.g. [R binning for Chi2](https://cran.r-project.org/web/packages/discretization/discretization.pdf) and [handling infinite values](https://help.palisade.com/v8/en/@RISK/1-Define/3-Fit/Chi-Squared-Binning.htm). \r\n\r\nPersonally, I am not proficient enough in statistics to propose and theoretically justify any particular binning strategy and its influence on the feature selection performance.", "created_at": "2024-12-12T17:03:07Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30469, "instance_id": "scikit-learn__scikit-learn-30469", "issue_numbers": ["30445", "29167"], "base_commit": "9a749bdcb2be578c387f00c067bade56e8ae7539", "patch": "diff --git a/doc/faq.rst b/doc/faq.rst\nindex 18132c7ad3095..dc8e45c78faa7 100644\n--- a/doc/faq.rst\n+++ b/doc/faq.rst\n@@ -360,6 +360,15 @@ long-term maintenance issues in open-source software, look at\n Using scikit-learn\n ------------------\n \n+How do I get started with scikit-learn?\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+If you are new to scikit-learn, or looking to strengthen your understanding,\n+we highly recommend the **scikit-learn MOOC (Massive Open Online Course)**.\n+\n+See our :ref:`External Resources, Videos and Talks page <external_resources>`\n+for more details.\n+\n What's the best way to get help on scikit-learn usage?\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n \ndiff --git a/doc/presentations.rst b/doc/presentations.rst\nindex 92f23c0fa26cb..0145e703e2cb9 100644\n--- a/doc/presentations.rst\n+++ b/doc/presentations.rst\n@@ -1,44 +1,75 @@\n+.. _external_resources:\n+\n ===========================================\n External Resources, Videos and Talks\n ===========================================\n \n-New to Scientific Python?\n-==========================\n-For those that are still new to the scientific Python ecosystem, we highly\n-recommend the `Python Scientific Lecture Notes\n-<https://scipy-lectures.org>`_. This will help you find your footing a\n-bit and will definitely improve your scikit-learn experience.  A basic\n-understanding of NumPy arrays is recommended to make the most of scikit-learn.\n+The scikit-learn MOOC\n+=====================\n \n-External Tutorials\n-===================\n+If you are new to scikit-learn, or looking to strengthen your understanding,\n+we highly recommend the **scikit-learn MOOC (Massive Open Online Course)**.\n \n-There are several online tutorials available which are geared toward\n-specific subject areas:\n+The MOOC, created and maintained by some of the scikit-learn core-contributors,\n+is **free of charge** and is designed to help learners of all levels master\n+machine learning using scikit-learn. It covers topics\n+from the fundamental machine learning concepts to more advanced areas like\n+predictive modeling pipelines and model evaluation.\n \n-- `Machine Learning for NeuroImaging in Python <https://nilearn.github.io/>`_\n-- `Machine Learning for Astronomical Data Analysis <https://github.com/astroML/sklearn_tutorial>`_\n+The course materials are available on the\n+`scikit-learn MOOC website <https://inria.github.io/scikit-learn-mooc/>`_.\n+\n+This course is also hosted on the `FUN platform\n+<https://www.fun-mooc.fr/en/courses/machine-learning-python-scikit-learn/>`_,\n+which additionally makes the content interactive without the need to install\n+anything, and gives access to a discussion forum.\n+\n+The videos are available on the\n+`Inria Learning Lab channel <https://www.youtube.com/@inrialearninglab>`_\n+in a\n+`playlist <https://www.youtube.com/playlist?list=PL2okA_2qDJ-m44KooOI7x8tu85wr4ez4f>`__.\n \n .. _videos:\n \n Videos\n ======\n \n-- An introduction to scikit-learn `Part\n-  I <https://conference.scipy.org/scipy2013/tutorial_detail.php?id=107>`_ and\n-  `Part II <https://conference.scipy.org/scipy2013/tutorial_detail.php?id=111>`_ at Scipy 2013\n-  by `Gael Varoquaux`_, `Jake Vanderplas`_  and `Olivier Grisel`_. Notebooks on\n-  `github <https://github.com/jakevdp/sklearn_scipy2013>`_.\n+- The `scikit-learn YouTube channel <https://www.youtube.com/@scikit-learn>`_\n+  features a\n+  `playlist <https://www.youtube.com/@scikit-learn/playlists>`__\n+  of videos\n+  showcasing talks by maintainers\n+  and community members.\n+\n+Older videos\n+------------\n+\n+- An introduction to scikit-learn at Scipy 2013\n+  by :user:`Gael Varoquaux <GaelVaroquaux>`,\n+  :user:`Jake Vanderplas <jakevdp>`  and\n+  :user:`Olivier Grisel <ogrisel>`.\n+\n+  Part I:\n+  `1 <https://www.youtube.com/watch?v=r4bRUvvlaBw>`__,\n+  `2 <https://www.youtube.com/watch?v=hlaMiXCRxB0>`__,\n+  `3 <https://www.youtube.com/watch?v=XS4TIGe7MaU>`__.\n+\n+  Part II:\n+  `1 <https://www.youtube.com/watch?v=uX4ZirOiWkw>`__,\n+  `2 <https://www.youtube.com/watch?v=kTLyp10FD60>`__.\n+\n+  Notebooks available on\n+  `github <https://github.com/jakevdp/sklearn_scipy2013>`__.\n \n - `Introduction to scikit-learn\n-  <http://videolectures.net/icml2010_varaquaux_scik/>`_ by `Gael Varoquaux`_ at\n-  ICML 2010\n+  <http://videolectures.net/icml2010_varaquaux_scik/>`_\n+  by :user:`Gael Varoquaux <GaelVaroquaux>` at ICML 2010\n \n   A three minute video from a very early stage of scikit-learn, explaining the\n   basic idea and approach we are following.\n \n - `Introduction to statistical learning with scikit-learn <https://archive.org/search.php?query=scikit-learn>`_\n-  by `Gael Varoquaux`_ at SciPy 2011\n+  by :user:`Gael Varoquaux <GaelVaroquaux>` at SciPy 2011\n \n   An extensive tutorial, consisting of four sessions of one hour.\n   The tutorial covers the basics of machine learning,\n@@ -47,27 +78,41 @@ Videos\n - `Statistical Learning for Text Classification with scikit-learn and NLTK\n   <https://pyvideo.org/video/417/pycon-2011--statistical-machine-learning-for-text>`_\n   (and `slides <https://www.slideshare.net/ogrisel/statistical-machine-learning-for-text-classification-with-scikitlearn-and-nltk>`_)\n-  by `Olivier Grisel`_ at PyCon 2011\n+  by :user:`Olivier Grisel <ogrisel>` at PyCon 2011\n \n   Thirty minute introduction to text classification. Explains how to\n   use NLTK and scikit-learn to solve real-world text classification\n   tasks and compares against cloud-based solutions.\n \n - `Introduction to Interactive Predictive Analytics in Python with scikit-learn <https://www.youtube.com/watch?v=Zd5dfooZWG4>`_\n-  by `Olivier Grisel`_ at PyCon 2012\n+  by :user:`Olivier Grisel <ogrisel>` at PyCon 2012\n \n   3-hours long introduction to prediction tasks using scikit-learn.\n \n - `scikit-learn - Machine Learning in Python <https://www.youtube.com/watch?v=cHZONQ2-x7I>`_\n-  by `Jake Vanderplas`_ at the 2012 PyData workshop at Google\n+  by :user:`Jake Vanderplas <jakevdp>` at the 2012 PyData workshop at Google\n \n   Interactive demonstration of some scikit-learn features. 75 minutes.\n \n-- `scikit-learn tutorial <https://www.youtube.com/watch?v=cHZONQ2-x7I>`_ by `Jake Vanderplas`_ at PyData NYC 2012\n+- `scikit-learn tutorial <https://www.youtube.com/watch?v=cHZONQ2-x7I>`_\n+  by :user:`Jake Vanderplas <jakevdp>` at PyData NYC 2012\n \n   Presentation using the online tutorial, 45 minutes.\n \n+New to Scientific Python?\n+==========================\n+\n+For those that are still new to the scientific Python ecosystem, we highly\n+recommend the `Python Scientific Lecture Notes\n+<https://scipy-lectures.org>`_. This will help you find your footing a\n+bit and will definitely improve your scikit-learn experience.  A basic\n+understanding of NumPy arrays is recommended to make the most of scikit-learn.\n+\n+External Tutorials\n+===================\n \n-.. _Gael Varoquaux: https://gael-varoquaux.info\n-.. _Jake Vanderplas: http://www.vanderplas.com\n-.. _Olivier Grisel: https://twitter.com/ogrisel\n+There are several online tutorials available which are geared toward\n+specific subject areas:\n+\n+- `Machine Learning for NeuroImaging in Python <https://nilearn.github.io/>`_\n+- `Machine Learning for Astronomical Data Analysis <https://github.com/astroML/sklearn_tutorial>`_\n", "test_patch": "", "problem_statement": "DOC add FAQ link to scikit-learn course\n### Describe the issue linked to the documentation\n\nGiven there are so many inquiries such as \"How do I get started with scikit-learn?\" let's add  a resource to the FAQ here:\r\nhttps://scikit-learn.org/stable/faq.html\r\n\r\nresource:\r\nhttps://inria.github.io/scikit-learn-mooc/appendix/datasets_intro.html\n\n### Suggest a potential alternative/fix\n\n_No response_\nDOC Beginners Tutorial\nWe need to have a section in our user guide introducing new users to a few concepts similar to what was present in our basics tutorial: https://github.com/scikit-learn/scikit-learn/blob/bc7e52ad7379fd8138418619a1cb0aeef07896a9/doc/tutorial/basic/tutorial.rst\r\n\r\nThis can go into our \"Getting Started\" or can be something linked from \"Getting Started\" page.\r\n\r\nWe should also link from our docs to the MOOC: https://inria.github.io/scikit-learn-mooc/\n", "hints_text": "+1 for linking to the MOOC material, but I would rather link to the main page rather than the appendix dedicated to exploratory data analysis of the datasets used elsewhere in the MOOC.\r\n\r\nhttps://inria.github.io/scikit-learn-mooc/index.html\nI'd like to work on this issue, if it's available.\nThank you for raising this issue! It's true that the current user guide can be a bit overwhelming for beginners who are new to machine learning.\r\n\r\nI really like the suggestion of creating a new section something like (\"Before you get started\"/\"Getting Started\") that includes content highlighted in the 'basics tutorial.' This could significantly improve the onboarding experience for new users.\r\n\r\nI'm familiar with scikit-learn and have experience writing technical documentation. I'd be happy to help develop this section to incorporate these introductory concepts.\r\n\r\nIs there a specific format or style guide for contributing to the user guide? I'm happy to follow any existing guidelines and collaborate with you on this effort.\nThe format / style guide would be the same as what we already have in the user guide. You might wanna have a look at our contributing guide, specifically the documentation part: https://scikit-learn.org/dev/developers/contributing.html#documentation\nI noted that the current [Getting Started](https://scikit-learn.org/dev/getting_started.html) (1) section outside User Guide covers basic commands re: model training and evaluation. \r\n\r\nWhile the [tutorial](https://github.com/scikit-learn/scikit-learn/blob/bc7e52ad7379fd8138418619a1cb0aeef07896a9/doc/tutorial/basic/tutorial.rst) (2) covers brief foundational ML theory.\r\n\r\nCombining (1) and (2) would give the users a more comprehensive starting point, and make them better prepared for the User Guide. I can get started on one section that covers that.\r\n\r\nIf you have any initial thoughts (topics, structure) & a timeline for this, please let me know. Thanks!\nHey can I work on this if this issue's open?\n@hoipranav this is not suitable as a first contribution. I suggest you start with easier issues or to continue pull requests that are marked as easy and stalled\nOkay @adrinjalali I'll surely look into some easier or stalled PR. Thank you!", "created_at": "2024-12-11T21:38:36Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30455, "instance_id": "scikit-learn__scikit-learn-30455", "issue_numbers": ["30247"], "base_commit": "446adff13b9af38404c694fed409e83b70b9c300", "patch": "diff --git a/doc/developers/maintainer.rst.template b/doc/developers/maintainer.rst.template\nindex a9877f7dd8c47..9c39d00775557 100644\n--- a/doc/developers/maintainer.rst.template\n+++ b/doc/developers/maintainer.rst.template\n@@ -118,6 +118,7 @@ Reference Steps\n         * [ ] Update the sklearn dev0 version in main branch\n         {%- endif %}\n         * [ ] Set the version number in the release branch\n+        * [ ] Generate the changelog in the release branch\n         * [ ] Check that the wheels for the release can be built successfully\n         * [ ] Merge the PR with `[cd build]` commit message to upload wheels to the staging repo\n         * [ ] Upload the wheels and source tarball to https://test.pypi.org\n@@ -125,8 +126,10 @@ Reference Steps\n         * [ ] Confirm bot detected at https://github.com/conda-forge/scikit-learn-feedstock\n               and wait for merge\n         * [ ] Upload the wheels and source tarball to PyPI\n+        {%- if key != \"rc\" %}\n         * [ ] Update news and what's new date in main branch\n         * [ ] Backport news and what's new date in release branch\n+        {%- endif %}\n         {%- if key == \"final\" %}\n         * [ ] Update symlink for stable in https://github.com/scikit-learn/scikit-learn.github.io\n         {%- endif %}\n@@ -139,17 +142,62 @@ Reference Steps\n         {%- endif %}\n \n     {% if key == \"rc\" %}\n-    - Create a PR from `main` and targeting `main` to increment the dev0 `__version__`\n-      variable in `sklearn/__init__.py`. This means while we are in the release\n-      candidate period, the latest stable is two version behind the `main` branch,\n-      instead of one. In this PR targeting `main`, you should also include a new what's\n-      new file under the `doc/whats_new/` directory so that we prepare the\n-      changelog for the next release.\n+    - Create a PR from `main` and targeting `main` to prepare for the next version. In\n+      this PR you need to:\n+      \n+      - Increment the dev0 `__version__` variable in `sklearn/__init__.py`. This means\n+        that while we are in the release candidate period, the latest stable is two\n+        versions behind the `main` branch, instead of one.\n+      \n+      - Include a new what's new file under the `doc/whats_new/` directory. Don't forget\n+        to add an entry for this new file in `doc/whats_new.rst`.\n+\n+      - Change the what's new file to the newly created one in the `filename` field of\n+        the `tool.towncrier` section in `pyproject.toml`. \n+\n     {% endif %}\n \n     - In the release branch, change the version number `__version__` in\n       `sklearn/__init__.py` to `{{ version_full }}`.\n \n+    - In the release branch, generate the changelog for the incoming version, i.e\n+      `doc/whats_new/{{ version_short }}.rst`.\n+      {%- if key == \"rc\" %}\n+      During the RC period we want to keep the fragments when we generate the changelog\n+      because we'll generate it again for the final release, including the changes that\n+      may happen in between:\n+\n+      .. prompt:: bash\n+\n+        towncrier build --keep --version {{ version_short}}.0\n+      {%- else -%}\n+      For a non RC release, push a commit where you:\n+      \n+      - generate the changelog, not keeping the fragments.\n+\n+      .. prompt:: bash\n+\n+        towncrier build --version {{ version_full}}\n+\n+      {%- if key == \"final\" %}\n+      - link the release highlights example\n+      {%- endif %}\n+      - add the list of contributor names. Suppose that the tag of the last release in\n+      the previous major/minor version is `{{ previous_tag }}`, then you can use the\n+      following command to retrieve the list of contributor names:\n+\n+      .. prompt:: bash\n+\n+        git shortlog -s {{ previous_tag }}.. |\n+          cut -f2- |\n+          sort --ignore-case |\n+          tr \"\\n\" \";\" |\n+          sed \"s/;/, /g;s/, $//\" |\n+          fold -s\n+\n+      Then create a PR targeting the `main` branch and cherry-pick this commit there.\n+      {%- endif %}\n+\n     - Trigger the wheel builder with the `[cd build]` commit marker. See also the\n       `workflow runs of the wheel builder\n       <https://github.com/scikit-learn/scikit-learn/actions/workflows/wheels.yml>`_.\n@@ -206,6 +254,12 @@ Reference Steps\n       https://github.com/conda-forge/scikit-learn-feedstock. If not, submit a PR for the\n       release, targeting the `{% if key == \"rc\" %}rc{% else %}main{% endif %}` branch.\n \n+      {%- if key == \"rc\" %}\n+      Make sure to update the PR such that it will be synchronized with the `main`\n+      branch. In particular, backport migrations that may have been added since the last\n+      release.\n+      {% endif %}\n+\n     - Trigger the `PyPI publishing workflow\n       <https://github.com/scikit-learn/scikit-learn/actions/workflows/publish_pypi.yml>`_\n       again, but this time to upload the artifacts to the real https://pypi.org/. To do\n@@ -246,24 +300,6 @@ Reference Steps\n           twine upload dist/*\n \n     {% if key != \"rc\" %}\n-    - In the `main` branch, edit the corresponding file in the `doc/whats_new` directory\n-      to update the release date\n-      {%- if key == \"final\" %}, link the release highlights example,{% endif %}\n-      and add the list of contributor names. Suppose that the tag of the last release in\n-      the previous major/minor version is `{{ previous_tag }}`, then you can use the\n-      following command to retrieve the list of contributor names:\n-\n-      .. prompt:: bash\n-\n-        git shortlog -s {{ previous_tag }}.. |\n-          cut -f2- |\n-          sort --ignore-case |\n-          tr \"\\n\" \";\" |\n-          sed \"s/;/, /g;s/, $//\" |\n-          fold -s\n-\n-      Then cherry-pick it in the release branch.\n-\n     - In the `main` branch, edit `doc/templates/index.html` to change the \"News\" section\n       in the landing page, along with the month of the release.\n       {%- if key == \"final\" %}\n", "test_patch": "", "problem_statement": "Notes to update the release process\nThis issue is used to consolidate the point that needs to be updated in the release process, notably due to the adoption of towncrier.\r\n\r\n### RC release process:\r\n\r\n- when requesting to bump the version number of `dev0` in `main`, we need to request changing the root RST file targeted by towncrier in the `pyproject.toml` file.\r\n- add that we should generate the changelog with `towncrier`: Generate the changelog with towncrier but keep the fragments: `towncrier build --keep --version 1.6.0` (*we should adapat the version number*)\n", "hints_text": "we also need a new step to generate the changelog from the fragments\nIndeed, without removing the fragment moreover.\nWe should also add more details for the conda-forge PR for release candidates since it's usually a bit more complicated. First it must target the rc branch and we need to sync with main and backport stuff like numpy 2 or new python versions that may have happen in between the last RC and this one that were only done in main.\nI also think that these 2 items from the RC check list are wrong:\r\n```\r\n[ ] Update news and what's new date in main branch\r\n[ ] Backport news and what's new date in release branch\r\n```\r\nalthough there's currently an issue because the what's new for 1.6 is not available on stable (but it is on dev)", "created_at": "2024-12-10T17:01:36Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30454, "instance_id": "scikit-learn__scikit-learn-30454", "issue_numbers": ["29107"], "base_commit": "6c3001574a88b1d5a026b81148e6492666cdd211", "patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.metrics/30454.fix.rst b/doc/whats_new/upcoming_changes/sklearn.metrics/30454.fix.rst\nnew file mode 100644\nindex 0000000000000..a53850e324e90\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.metrics/30454.fix.rst\n@@ -0,0 +1,3 @@\n+- Fix regression when scikit-learn metric called on PyTorch CPU tensors would\n+  raise an error (with array API dispatch disabled which is the default).\n+  By :user:`Lo\u00efc Est\u00e8ve <lesteve>`\ndiff --git a/sklearn/utils/_array_api.py b/sklearn/utils/_array_api.py\nindex b2b4f88fa218f..65503a0674a70 100644\n--- a/sklearn/utils/_array_api.py\n+++ b/sklearn/utils/_array_api.py\n@@ -130,10 +130,17 @@ def _check_array_api_dispatch(array_api_dispatch):\n \n def _single_array_device(array):\n     \"\"\"Hardware device where the array data resides on.\"\"\"\n-    if isinstance(array, (numpy.ndarray, numpy.generic)) or not hasattr(\n-        array, \"device\"\n+    if (\n+        isinstance(array, (numpy.ndarray, numpy.generic))\n+        or not hasattr(array, \"device\")\n+        # When array API dispatch is disabled, we expect the scikit-learn code\n+        # to use np.asarray so that the resulting NumPy array will implicitly use the\n+        # CPU. In this case, scikit-learn should stay as device neutral as possible,\n+        # hence the use of `device=None` which is accepted by all libraries, before\n+        # and after the expected conversion to NumPy via np.asarray.\n+        or not get_config()[\"array_api_dispatch\"]\n     ):\n-        return \"cpu\"\n+        return None\n     else:\n         return array.device\n \ndiff --git a/sklearn/utils/estimator_checks.py b/sklearn/utils/estimator_checks.py\nindex 7416216dda520..f68fd8d091119 100644\n--- a/sklearn/utils/estimator_checks.py\n+++ b/sklearn/utils/estimator_checks.py\n@@ -1113,6 +1113,40 @@ def check_array_api_input(\n         \"transform\",\n     )\n \n+    try:\n+        np.asarray(X_xp)\n+        np.asarray(y_xp)\n+        # TODO There are a few errors in SearchCV with array-api-strict because\n+        # we end up doing X[train_indices] where X is an array-api-strict array\n+        # and train_indices is a numpy array. array-api-strict insists\n+        # train_indices should be an array-api-strict array. On the other hand,\n+        # all the array API libraries (PyTorch, jax, CuPy) accept indexing with a\n+        # numpy array. This is probably not worth doing anything about for\n+        # now since array-api-strict seems a bit too strict ...\n+        numpy_asarray_works = xp.__name__ != \"array_api_strict\"\n+\n+    except TypeError:\n+        # PyTorch with CUDA device and CuPy raise TypeError consistently.\n+        # Exception type may need to be updated in the future for other\n+        # libraries.\n+        numpy_asarray_works = False\n+\n+    if numpy_asarray_works:\n+        # In this case, array_api_dispatch is disabled and we rely on np.asarray\n+        # being called to convert the non-NumPy inputs to NumPy arrays when needed.\n+        est_fitted_with_as_array = clone(est).fit(X_xp, y_xp)\n+        # We only do a smoke test for now, in order to avoid complicating the\n+        # test function even further.\n+        for method_name in methods:\n+            method = getattr(est_fitted_with_as_array, method_name, None)\n+            if method is None:\n+                continue\n+\n+            if method_name == \"score\":\n+                method(X_xp, y_xp)\n+            else:\n+                method(X_xp)\n+\n     for method_name in methods:\n         method = getattr(est, method_name, None)\n         if method is None:\n", "test_patch": "diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py\nindex 0b7a47b0f12da..ef8e6ebb2ac2a 100644\n--- a/sklearn/metrics/tests/test_common.py\n+++ b/sklearn/metrics/tests/test_common.py\n@@ -1817,6 +1817,40 @@ def check_array_api_metric(\n     if isinstance(multioutput, np.ndarray):\n         metric_kwargs[\"multioutput\"] = xp.asarray(multioutput, device=device)\n \n+    # When array API dispatch is disabled, and np.asarray works (for example PyTorch\n+    # with CPU device), calling the metric function with such numpy compatible inputs\n+    # should work (albeit by implicitly converting to numpy arrays instead of\n+    # dispatching to the array library).\n+    try:\n+        np.asarray(a_xp)\n+        np.asarray(b_xp)\n+        numpy_as_array_works = True\n+    except TypeError:\n+        # PyTorch with CUDA device and CuPy raise TypeError consistently.\n+        # Exception type may need to be updated in the future for other\n+        # libraries.\n+        numpy_as_array_works = False\n+\n+    if numpy_as_array_works:\n+        metric_xp = metric(a_xp, b_xp, **metric_kwargs)\n+        assert_allclose(\n+            metric_xp,\n+            metric_np,\n+            atol=_atol_for_type(dtype_name),\n+        )\n+        metric_xp_mixed_1 = metric(a_np, b_xp, **metric_kwargs)\n+        assert_allclose(\n+            metric_xp_mixed_1,\n+            metric_np,\n+            atol=_atol_for_type(dtype_name),\n+        )\n+        metric_xp_mixed_2 = metric(a_xp, b_np, **metric_kwargs)\n+        assert_allclose(\n+            metric_xp_mixed_2,\n+            metric_np,\n+            atol=_atol_for_type(dtype_name),\n+        )\n+\n     with config_context(array_api_dispatch=True):\n         metric_xp = metric(a_xp, b_xp, **metric_kwargs)\n \ndiff --git a/sklearn/utils/tests/test_array_api.py b/sklearn/utils/tests/test_array_api.py\nindex 82b6a7df557e5..d76ef4838e37e 100644\n--- a/sklearn/utils/tests/test_array_api.py\n+++ b/sklearn/utils/tests/test_array_api.py\n@@ -248,6 +248,7 @@ def test_device_none_if_no_input():\n     assert device(None, \"name\") is None\n \n \n+@skip_if_array_api_compat_not_configured\n def test_device_inspection():\n     class Device:\n         def __init__(self, name):\n@@ -273,18 +274,26 @@ def __init__(self, device_name):\n     with pytest.raises(TypeError):\n         hash(Array(\"device\").device)\n \n-    # Test raise if on different devices\n+    # If array API dispatch is disabled the device should be ignored. Erroring\n+    # early for different devices would prevent the np.asarray conversion to\n+    # happen. For example, `r2_score(np.ones(5), torch.ones(5))` should work\n+    # fine with array API disabled.\n+    assert device(Array(\"cpu\"), Array(\"mygpu\")) is None\n+\n+    # Test that ValueError is raised if on different devices and array API dispatch is\n+    # enabled.\n     err_msg = \"Input arrays use different devices: cpu, mygpu\"\n-    with pytest.raises(ValueError, match=err_msg):\n-        device(Array(\"cpu\"), Array(\"mygpu\"))\n+    with config_context(array_api_dispatch=True):\n+        with pytest.raises(ValueError, match=err_msg):\n+            device(Array(\"cpu\"), Array(\"mygpu\"))\n \n-    # Test expected value is returned otherwise\n-    array1 = Array(\"device\")\n-    array2 = Array(\"device\")\n+        # Test expected value is returned otherwise\n+        array1 = Array(\"device\")\n+        array2 = Array(\"device\")\n \n-    assert array1.device == device(array1)\n-    assert array1.device == device(array1, array2)\n-    assert array1.device == device(array1, array1, array2)\n+        assert array1.device == device(array1)\n+        assert array1.device == device(array1, array2)\n+        assert array1.device == device(array1, array1, array2)\n \n \n # TODO: add cupy to the list of libraries once the the following upstream issue\n@@ -553,7 +562,7 @@ def test_get_namespace_and_device():\n     namespace, is_array_api, device = get_namespace_and_device(some_torch_tensor)\n     assert namespace is get_namespace(some_numpy_array)[0]\n     assert not is_array_api\n-    assert device.type == \"cpu\"\n+    assert device is None\n \n     # Otherwise, expose the torch namespace and device via array API compat\n     # wrapper.\n@@ -621,8 +630,8 @@ def test_sparse_device(csr_container, dispatch):\n     try:\n         with config_context(array_api_dispatch=dispatch):\n             assert device(a, b) is None\n-            assert device(a, numpy.array([1])) == \"cpu\"\n+            assert device(a, numpy.array([1])) is None\n             assert get_namespace_and_device(a, b)[2] is None\n-            assert get_namespace_and_device(a, numpy.array([1]))[2] == \"cpu\"\n+            assert get_namespace_and_device(a, numpy.array([1]))[2] is None\n     except ImportError:\n         raise SkipTest(\"array_api_compat is not installed\")\n", "problem_statement": "Incorrect invalid device error introduced in #25956\n### Describe the bug\r\n\r\n#25956 introduced a new `sklearn.utils._array_api._check_device_cpu` function to test whether a tensor is on CPU. However, the implementation of the test, which is `device not in {\"cpu\", None}`, is incorrect -- the device will actually not be a string, but `device(type='cpu')`. Therefore, you should attempt to get the `type` attr, and use that if available.\r\n\r\n### Steps/Code to Reproduce\r\n\r\nYou can view a sample error here:\r\nhttps://github.com/fastai/fastai/actions/runs/9232979440/job/25404873935\r\n\r\n### Expected Results\r\n\r\n`ValueError: Unsupported device for NumPy: device(type='cpu')`  should not be thrown.\r\n\r\n### Actual Results\r\n\r\n```\r\nFile /opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/sklearn/utils/_array_api.py:308, in _check_device_cpu(device)\r\n    306 def _check_device_cpu(device):  # noqa\r\n    307     if device not in {\"cpu\", None}:\r\n--> 308         raise ValueError(f\"Unsupported device for NumPy: {device!r}\")\r\n\r\nValueError: Unsupported device for NumPy: device(type='cpu')\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nI've seen this on multiple Linux and Mac versions. My current Mac version:\r\n\r\n\r\nSystem:\r\n    python: 3.11.8 (main, Feb 26 2024, 15:36:12) [Clang 14.0.6 ]\r\nexecutable: /Users/jhoward/miniconda3/bin/python\r\n   machine: macOS-14.3.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.2\r\n          pip: 23.3.1\r\n   setuptools: 68.2.2\r\n        numpy: 1.26.4\r\n        scipy: 1.13.0\r\n       Cython: None\r\n       pandas: 2.2.1\r\n   matplotlib: 3.8.4\r\n       joblib: 1.4.0\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       filepath: /Users/jhoward/miniconda3/lib/libopenblasp-r0.3.21.dylib\r\n         prefix: libopenblas\r\n       user_api: blas\r\n   internal_api: openblas\r\n        version: 0.3.21\r\n    num_threads: 8\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n\r\n       filepath: /Users/jhoward/miniconda3/lib/libomp.dylib\r\n         prefix: libomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 8\r\n```\r\n```\r\n\n", "hints_text": "Do you have a minimal reproducer.\r\n\r\nLooking at our code, I think that we expect to have a call to the `sklearn.utils._array_api.device` that is going to return the string `\"cpu\"`. So it would be interesting to understand what is generating the `device` object to have a proper regression test.\r\n\r\nI'm not the most familiar with the array API work so, ping @betatim and @ogrisel.\nLooking at the code, this function `_check_device_cpu` seems to only be used for `_NumPyAPIWrapper` which is not meant to be called in torch inputs I think. So we need to understand how you ended up with an `xp` module that is an instance of the `_NumPyAPIWrapper` but with a `device` variable that holds a torch device (instead of `None`).\nI tried to craft a minimal reproducer by mixing numpy and torch inputs as follows:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import array_api_compat.torch as xp\r\n>>> import sklearn\r\n>>> sklearn.set_config(array_api_dispatch=True)\r\n>>> from sklearn.metrics import r2_score\r\n>>> r2_score(xp.arange(10, device=\"mps\"), xp.arange(10, device=\"mps\"), sample_weight=xp.asarray(range(10), device=\"mps\"))\r\n1.0\r\n>>> r2_score(np.arange(10), xp.arange(10, device=\"mps\"), sample_weight=xp.arange(10, device=\"mps\"))\r\nTraceback (most recent call last):\r\n  Cell In[16], line 1\r\n    r2_score(np.arange(10), xp.arange(10, device=\"mps\"), sample_weight=xp.arange(10, device=\"mps\"))\r\n  File ~/code/scikit-learn/sklearn/utils/_param_validation.py:213 in wrapper\r\n    return func(*args, **kwargs)\r\n  File ~/code/scikit-learn/sklearn/metrics/_regression.py:1214 in r2_score\r\n    xp, _, device_ = get_namespace_and_device(\r\n  File ~/code/scikit-learn/sklearn/utils/_array_api.py:572 in get_namespace_and_device\r\n    *get_namespace(*array_list, **skip_remove_kwargs),\r\n  File ~/code/scikit-learn/sklearn/utils/_array_api.py:553 in get_namespace\r\n    namespace, is_array_api_compliant = array_api_compat.get_namespace(*arrays), True\r\n  File ~/miniforge3/envs/dev/lib/python3.11/site-packages/array_api_compat/common/_helpers.py:372 in array_namespace\r\n    raise TypeError(f\"Multiple namespaces for array inputs: {namespaces}\")\r\nTypeError: Multiple namespaces for array inputs: {<module 'array_api_compat.numpy' from '/Users/ogrisel/miniforge3/envs/dev/lib/python3.11/site-packages/array_api_compat/numpy/__init__.py'>, <module 'array_api_compat.torch' from '/Users/ogrisel/miniforge3/envs/dev/lib/python3.11/site-packages/array_api_compat/torch/__init__.py'>}\r\n```\r\n\r\nbut it's raising the expected error message that states that `r2_score` should not be called with mixed-type inputs (as of now, we might relax this in the future if we have a good reason to do so).\nFrom the traceback I also tried:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import sklearn\r\n>>> sklearn.set_config(array_api_dispatch=True)\r\n>>> from sklearn.metrics import r2_score\r\n>>> import torch\r\n>>> x1, x2 = torch.randn(20, 5), torch.randn(20, 5)\r\n>>> r2_score(x2.view(-1), x1.view(-1))\r\n-0.8340672254562378\r\n```\r\n\r\nbut I cannot reproduce either.\nOk I understand, it's happening when array API dispatch is disabled and regular numpy code is expected:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import sklearn\r\n>>> sklearn.set_config(array_api_dispatch=False)\r\n>>> from sklearn.metrics import r2_score\r\n>>> import torch\r\n>>> x1, x2 = torch.randn(20, 5), torch.randn(20, 5)\r\n>>> r2_score(x2.view(-1), x1.view(-1))\r\nTraceback (most recent call last):\r\n  Cell In[27], line 7\r\n    r2_score(x2.view(-1), x1.view(-1))\r\n  File ~/code/scikit-learn/sklearn/utils/_param_validation.py:213 in wrapper\r\n    return func(*args, **kwargs)\r\n  File ~/code/scikit-learn/sklearn/metrics/_regression.py:1242 in r2_score\r\n    return _assemble_r2_explained_variance(\r\n  File ~/code/scikit-learn/sklearn/metrics/_regression.py:897 in _assemble_r2_explained_variance\r\n    output_scores = xp.ones([n_outputs], device=device, dtype=dtype)\r\n  File ~/code/scikit-learn/sklearn/utils/_array_api.py:314 in wrapped_func\r\n    _check_device_cpu(kwargs.pop(\"device\", None))\r\n  File ~/code/scikit-learn/sklearn/utils/_array_api.py:308 in _check_device_cpu\r\n    raise ValueError(f\"Unsupported device for NumPy: {device!r}\")\r\nValueError: Unsupported device for NumPy: device(type='cpu')\r\n```\r\n\r\nso this is indeed a regression: when array API dispatch is disabled, the usual implicit conversion from torch CPU tensors to NumPy arrays should still happen and for some reason, we get a non-None device where we don't expect it. I need to investigate.\r\n\nI think I get it. It's `get_namespace_and_device` that is wrong when array API dispatch is disabled. I will open a PR with a non-regression test.\n@ogrisel is it possible that this bug has reappeared? I'm getting the same error again here:\r\nhttps://github.com/AnswerDotAI/fastcore/actions/runs/12251169586/job/34175356676\n@jph00, indeed it seems like a regression in scikit-learn 1.6. I am taking a closer look.\nReopening. Indeed, we made some changes in #29476  (including in the non-regression test named `test_get_namespace_and_device`) without realizing that it would break calls like `r2_score(torch.ones(3), torch.ones(3))` when array API dispatch is disabled (default). In this case want to rely on the regular `np.asarray` implicit conversion instead.\r\n\r\nWe need to fix it (either in the `sklearn.utils._array_api.device` function or in the `sklearn.utils._array_api.get_namespace_and_device` function. \r\n\r\nThis time, add a non-regression test that directly calls `r2_score(torch.ones(3), torch.ones(3))` (or other metrics functions) with array API dispatch disabled and check that it works. The intent of the test will be clearer and we won't risk changing it without understanding the consequence...", "created_at": "2024-12-10T16:05:08Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30451, "instance_id": "scikit-learn__scikit-learn-30451", "issue_numbers": ["30447"], "base_commit": "8481e681128822050e996c3ef6b65d3801d102f5", "patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.model_selection/30451.fix.rst b/doc/whats_new/upcoming_changes/sklearn.model_selection/30451.fix.rst\nnew file mode 100644\nindex 0000000000000..5ebfb5992d832\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.model_selection/30451.fix.rst\n@@ -0,0 +1,3 @@\n+- :func:`~model_selection.cross_validate`, :func:`~model_selection.cross_val_predict`,\n+  and :func:`~model_selection.cross_val_score` now accept `params=None` when metadata\n+  routing is enabled. By `Adrin Jalali`_\ndiff --git a/sklearn/model_selection/_validation.py b/sklearn/model_selection/_validation.py\nindex 7d38182911fb8..d5984d2454a4c 100644\n--- a/sklearn/model_selection/_validation.py\n+++ b/sklearn/model_selection/_validation.py\n@@ -343,7 +343,7 @@ def cross_validate(\n     _check_groups_routing_disabled(groups)\n \n     X, y = indexable(X, y)\n-\n+    params = {} if params is None else params\n     cv = check_cv(cv, y, classifier=is_classifier(estimator))\n \n     scorers = check_scoring(\n@@ -1172,6 +1172,7 @@ def cross_val_predict(\n     \"\"\"\n     _check_groups_routing_disabled(groups)\n     X, y = indexable(X, y)\n+    params = {} if params is None else params\n \n     if _routing_enabled():\n         # For estimators, a MetadataRouter is created in get_metadata_routing\n", "test_patch": "diff --git a/sklearn/model_selection/tests/test_validation.py b/sklearn/model_selection/tests/test_validation.py\nindex 2d579772b1fbe..73156c2a25337 100644\n--- a/sklearn/model_selection/tests/test_validation.py\n+++ b/sklearn/model_selection/tests/test_validation.py\n@@ -2539,6 +2539,27 @@ def test_groups_with_routing_validation(func, extra_args):\n         )\n \n \n+@pytest.mark.parametrize(\n+    \"func, extra_args\",\n+    [\n+        (cross_validate, {}),\n+        (cross_val_score, {}),\n+        (cross_val_predict, {}),\n+        (learning_curve, {}),\n+        (permutation_test_score, {}),\n+        (validation_curve, {\"param_name\": \"alpha\", \"param_range\": np.array([1])}),\n+    ],\n+)\n+@config_context(enable_metadata_routing=True)\n+def test_cross_validate_params_none(func, extra_args):\n+    \"\"\"Test that no errors are raised when passing `params=None`, which is the\n+    default value.\n+    Non-regression test for: https://github.com/scikit-learn/scikit-learn/issues/30447\n+    \"\"\"\n+    X, y = make_classification(n_samples=100, n_classes=2, random_state=0)\n+    func(estimator=ConsumingClassifier(), X=X, y=y, **extra_args)\n+\n+\n @pytest.mark.parametrize(\n     \"func, extra_args\",\n     [\n", "problem_statement": "`cross_validate` raises an exception when metadata routing is enabled\n### Describe the bug\n\nIn the latest release (v1.6.0), `cross_validate` raises an exception when using it with metadata routing enabled. This is because `params` dict gets unpacked even if `None`, which is the default value. See this line:\r\n\r\nhttps://github.com/scikit-learn/scikit-learn/blob/76ae0a539a0e87145c9f6fedcd7033494082fa17/sklearn/model_selection/_validation.py#L375\n\n### Steps/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nimport sklearn\r\nfrom sklearn.model_selection import cross_validate\r\nfrom sklearn.model_selection.tests.test_validation import MockClassifier\r\n\r\nsklearn.set_config(enable_metadata_routing=True)\r\n\r\nX = np.ones((10, 2))\r\ny = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\r\n\r\nclf = MockClassifier()\r\ncross_validate(clf, X, y)\r\n```\n\n### Expected Results\n\nNo exception being raised.\n\n### Actual Results\n\n```python-traceback\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<redacted>/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"<redacted>/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 375, in cross_validate\r\n    routed_params = process_routing(router, \"fit\", **params)\r\nTypeError: sklearn.utils._metadata_requests.process_routing() argument after ** must be a mapping, not NoneType\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\r\nexecutable: <redacted>/bin/python\r\n   machine: Linux-6.5.13netflix-g77293087f291-x86_64-with-glibc2.35\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.0\r\n          pip: None\r\n   setuptools: 75.6.0\r\n        numpy: 1.26.4\r\n        scipy: 1.14.1\r\n       Cython: None\r\n       pandas: 2.2.3\r\n   matplotlib: 3.9.3\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 4\r\n         prefix: libopenblas\r\n       filepath: /root/pycharm_projects/evaluations/.venv/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: Cooperlake\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 4\r\n         prefix: libscipy_openblas\r\n       filepath: /root/pycharm_projects/evaluations/.venv/lib/python3.10/site-packages/scipy.libs/libscipy_openblas-c128ec02.so\r\n        version: 0.3.27.dev\r\nthreading_layer: pthreads\r\n   architecture: Cooperlake\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 4\r\n         prefix: libgomp\r\n       filepath: <redacted>/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n```\n```\n\n", "hints_text": "Hi @bryant1410,\r\n\r\nthanks for reporting. I can re-create and agree it is an issue.\r\n\r\nI think we should either not error at all or catch it to raise a more tailored error message. I would tend towards the first option, because users might have enabled metadata routing for an entirely different purpose and only happen to also run a (possibly unrelated) cross validation in the same program without wanting to route any metadata.\r\n\r\nWhat do you think, @adrinjalali?", "created_at": "2024-12-10T09:38:54Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30443, "instance_id": "scikit-learn__scikit-learn-30443", "issue_numbers": ["30442"], "base_commit": "7f0215f5bee45a5c74728a1aaf37b58b12d4f3e6", "patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.decomposition/30443.feature.rst b/doc/whats_new/upcoming_changes/sklearn.decomposition/30443.feature.rst\nnew file mode 100644\nindex 0000000000000..5678039b69065\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.decomposition/30443.feature.rst\n@@ -0,0 +1,4 @@\n+- :class:`~sklearn.decomposition.DictionaryLearning`,\n+  :class:`~sklearn.decomposition.SparseCoder`  and\n+  :class:`~sklearn.decomposition.MiniBatchDictionaryLearning` now have a\n+  ``inverse_transform`` method. By :user:`R\u00e9mi Flamary <rflamary>`\ndiff --git a/sklearn/decomposition/_dict_learning.py b/sklearn/decomposition/_dict_learning.py\nindex 7410eeb4405df..282376550de24 100644\n--- a/sklearn/decomposition/_dict_learning.py\n+++ b/sklearn/decomposition/_dict_learning.py\n@@ -1142,6 +1142,44 @@ def transform(self, X):\n         check_is_fitted(self)\n         return self._transform(X, self.components_)\n \n+    def _inverse_transform(self, code, dictionary):\n+        \"\"\"Private method allowing to accommodate both DictionaryLearning and\n+        SparseCoder.\"\"\"\n+        code = check_array(code)\n+        # compute number of expected features in code\n+        expected_n_components = dictionary.shape[0]\n+        if self.split_sign:\n+            expected_n_components += expected_n_components\n+        if not code.shape[1] == expected_n_components:\n+            raise ValueError(\n+                \"The number of components in the code is different from the \"\n+                \"number of components in the dictionary.\"\n+                f\"Expected {expected_n_components}, got {code.shape[1]}.\"\n+            )\n+        if self.split_sign:\n+            n_samples, n_features = code.shape\n+            n_features //= 2\n+            code = code[:, :n_features] - code[:, n_features:]\n+\n+        return code @ dictionary\n+\n+    def inverse_transform(self, X):\n+        \"\"\"Transform data back to its original space.\n+\n+        Parameters\n+        ----------\n+        X : array-like of shape (n_samples, n_components)\n+            Data to be transformed back. Must have the same number of\n+            components as the data used to train the model.\n+\n+        Returns\n+        -------\n+        X_new : ndarray of shape (n_samples, n_features)\n+            Transformed data.\n+        \"\"\"\n+        check_is_fitted(self)\n+        return self._inverse_transform(X, self.components_)\n+\n \n class SparseCoder(_BaseSparseCoding, BaseEstimator):\n     \"\"\"Sparse coding.\n@@ -1329,6 +1367,22 @@ def transform(self, X, y=None):\n         \"\"\"\n         return super()._transform(X, self.dictionary)\n \n+    def inverse_transform(self, X):\n+        \"\"\"Transform data back to its original space.\n+\n+        Parameters\n+        ----------\n+        X : array-like of shape (n_samples, n_components)\n+            Data to be transformed back. Must have the same number of\n+            components as the data used to train the model.\n+\n+        Returns\n+        -------\n+        X_new : ndarray of shape (n_samples, n_features)\n+            Transformed data.\n+        \"\"\"\n+        return self._inverse_transform(X, self.dictionary)\n+\n     def __sklearn_tags__(self):\n         tags = super().__sklearn_tags__()\n         tags.requires_fit = False\n", "test_patch": "diff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py\nindex f52c851012481..717c56d0abdbe 100644\n--- a/sklearn/decomposition/tests/test_dict_learning.py\n+++ b/sklearn/decomposition/tests/test_dict_learning.py\n@@ -202,10 +202,16 @@ def test_dict_learning_reconstruction():\n     )\n     code = dico.fit(X).transform(X)\n     assert_array_almost_equal(np.dot(code, dico.components_), X)\n+    assert_array_almost_equal(dico.inverse_transform(code), X)\n \n     dico.set_params(transform_algorithm=\"lasso_lars\")\n     code = dico.transform(X)\n     assert_array_almost_equal(np.dot(code, dico.components_), X, decimal=2)\n+    assert_array_almost_equal(dico.inverse_transform(code), X, decimal=2)\n+\n+    # test error raised for wrong code size\n+    with pytest.raises(ValueError, match=\"Expected 12, got 11.\"):\n+        dico.inverse_transform(code[:, :-1])\n \n     # used to test lars here too, but there's no guarantee the number of\n     # nonzero atoms is right.\n@@ -268,6 +274,8 @@ def test_dict_learning_split():\n         n_components, transform_algorithm=\"threshold\", random_state=0\n     )\n     code = dico.fit(X).transform(X)\n+    Xr = dico.inverse_transform(code)\n+\n     dico.split_sign = True\n     split_code = dico.transform(X)\n \n@@ -275,6 +283,9 @@ def test_dict_learning_split():\n         split_code[:, :n_components] - split_code[:, n_components:], code\n     )\n \n+    Xr2 = dico.inverse_transform(split_code)\n+    assert_array_almost_equal(Xr, Xr2)\n+\n \n def test_dict_learning_online_shapes():\n     rng = np.random.RandomState(0)\n@@ -591,9 +602,12 @@ def test_sparse_coder_estimator():\n     V /= np.sum(V**2, axis=1)[:, np.newaxis]\n     coder = SparseCoder(\n         dictionary=V, transform_algorithm=\"lasso_lars\", transform_alpha=0.001\n-    ).transform(X)\n-    assert not np.all(coder == 0)\n-    assert np.sqrt(np.sum((np.dot(coder, V) - X) ** 2)) < 0.1\n+    )\n+    code = coder.fit_transform(X)\n+    Xr = coder.inverse_transform(code)\n+    assert not np.all(code == 0)\n+    assert np.sqrt(np.sum((np.dot(code, V) - X) ** 2)) < 0.1\n+    np.testing.assert_allclose(Xr, np.dot(code, V))\n \n \n def test_sparse_coder_estimator_clone():\n", "problem_statement": "Missing `inverse_transform` in `DictionaryLearning`and `SparseCoder`\n### Describe the workflow you want to enable\n\nThe method is currently missing in those two classes which prevent doing a loop over all Linear decomposition methods when evaluation them for denoising for instance. \n\n### Describe your proposed solution\n\nI propose to implement the method in the class `_BaseSparseCoding` from https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/decomposition/_dict_learning.py\r\n\r\nI have an implementation with updated tests that I will propose as PR when the issue is created. Ping to @agramfort with whom I already discussed about this.\n\n### Describe alternatives you've considered, if relevant\n\nthe documentation gives the following expale\r\n```python \r\nX_hat = X_transformed @ dict_learner.components_ \r\n```\r\nwhich is OK but requires the user to know about `components_` and to do that to all linear decomposition methods is tested in a loop  (the others have all implemented `inverse_transform`). Implementing the method closes a missing part of the API and would be better in my opinion.\n\n### Additional context\n\n_No response_\n", "hints_text": "", "created_at": "2024-12-09T16:29:08Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30428, "instance_id": "scikit-learn__scikit-learn-30428", "issue_numbers": ["30427"], "base_commit": "1e6a81f322f1821cc605a18b08fcc198c7d63c97", "patch": "diff --git a/doc/api_reference.py b/doc/api_reference.py\nindex b7bbeb3d3643f..7c81887f48f36 100644\n--- a/doc/api_reference.py\n+++ b/doc/api_reference.py\n@@ -549,7 +549,7 @@ def _get_submodule(module_name, submodule_name):\n         ],\n     },\n     \"sklearn.kernel_approximation\": {\n-        \"short_summary\": \"Isotonic regression.\",\n+        \"short_summary\": \"Kernel approximation.\",\n         \"description\": _get_guide(\"kernel_approximation\"),\n         \"sections\": [\n             {\n", "test_patch": "", "problem_statement": "Fix incorrect short_summary for sklearn.kernel_approximation module\n### Describe the issue linked to the documentation\n\nThe short_summary for the sklearn.kernel_approximation module is currently set to \"Isotonic regression,\" which is incorrect.\n\n### Suggest a potential alternative/fix\n\nUpdate the short_summary for sklearn.kernel_approximation.\n", "hints_text": "", "created_at": "2024-12-07T19:35:36Z"}
{"repo": "scikit-learn/scikit-learn", "pull_number": 30406, "instance_id": "scikit-learn__scikit-learn-30406", "issue_numbers": ["30364"], "base_commit": "c9fa7d4d2662ff0ef22cdcb0bd50d3a98c58a679", "patch": "diff --git a/doc/whats_new/upcoming_changes/sklearn.pipeline/30406.enhancement.rst b/doc/whats_new/upcoming_changes/sklearn.pipeline/30406.enhancement.rst\nnew file mode 100644\nindex 0000000000000..a1b6ac60078eb\n--- /dev/null\n+++ b/doc/whats_new/upcoming_changes/sklearn.pipeline/30406.enhancement.rst\n@@ -0,0 +1,4 @@\n+- Expose the ``verbose_feature_names_out`` argument in the\n+  :func:`pipeline.make_union` function, allowing users to control\n+  feature name uniqueness in the :class:`pipeline.FeatureUnion`.\n+  By :user:`Abhijeetsingh Meena <Ethan0456>`.\ndiff --git a/sklearn/pipeline.py b/sklearn/pipeline.py\nindex 9ff8a3549ef28..24b988a83a0ab 100644\n--- a/sklearn/pipeline.py\n+++ b/sklearn/pipeline.py\n@@ -2114,7 +2114,9 @@ def get_metadata_routing(self):\n         return router\n \n \n-def make_union(*transformers, n_jobs=None, verbose=False):\n+def make_union(\n+    *transformers, n_jobs=None, verbose=False, verbose_feature_names_out=True\n+):\n     \"\"\"Construct a :class:`FeatureUnion` from the given transformers.\n \n     This is a shorthand for the :class:`FeatureUnion` constructor; it does not\n@@ -2140,6 +2142,10 @@ def make_union(*transformers, n_jobs=None, verbose=False):\n         If True, the time elapsed while fitting each transformer will be\n         printed as it is completed.\n \n+    verbose_feature_names_out : bool, default=True\n+        If True, the feature names generated by `get_feature_names_out` will\n+        include prefixes derived from the transformer names.\n+\n     Returns\n     -------\n     f : FeatureUnion\n@@ -2159,4 +2165,9 @@ def make_union(*transformers, n_jobs=None, verbose=False):\n      FeatureUnion(transformer_list=[('pca', PCA()),\n                                    ('truncatedsvd', TruncatedSVD())])\n     \"\"\"\n-    return FeatureUnion(_name_estimators(transformers), n_jobs=n_jobs, verbose=verbose)\n+    return FeatureUnion(\n+        _name_estimators(transformers),\n+        n_jobs=n_jobs,\n+        verbose=verbose,\n+        verbose_feature_names_out=verbose_feature_names_out,\n+    )\n", "test_patch": "diff --git a/sklearn/tests/test_pipeline.py b/sklearn/tests/test_pipeline.py\nindex d7a201f3abf6f..bc9f372421d82 100644\n--- a/sklearn/tests/test_pipeline.py\n+++ b/sklearn/tests/test_pipeline.py\n@@ -603,6 +603,44 @@ def test_make_union_kwargs():\n         make_union(pca, mock, transformer_weights={\"pca\": 10, \"Transf\": 1})\n \n \n+def create_mock_transformer(base_name, n_features=3):\n+    \"\"\"Helper to create a mock transformer with custom feature names.\"\"\"\n+    mock = Transf()\n+    mock.get_feature_names_out = lambda input_features: [\n+        f\"{base_name}{i}\" for i in range(n_features)\n+    ]\n+    return mock\n+\n+\n+def test_make_union_passes_verbose_feature_names_out():\n+    # Test that make_union passes verbose_feature_names_out\n+    # to the FeatureUnion.\n+    X = iris.data\n+    y = iris.target\n+\n+    pca = PCA()\n+    mock = create_mock_transformer(\"transf\")\n+    union = make_union(pca, mock, verbose_feature_names_out=False)\n+\n+    assert not union.verbose_feature_names_out\n+\n+    fu_union = make_union(pca, mock, verbose_feature_names_out=True)\n+    fu_union.fit(X, y)\n+\n+    assert_array_equal(\n+        [\n+            \"pca__pca0\",\n+            \"pca__pca1\",\n+            \"pca__pca2\",\n+            \"pca__pca3\",\n+            \"transf__transf0\",\n+            \"transf__transf1\",\n+            \"transf__transf2\",\n+        ],\n+        fu_union.get_feature_names_out(),\n+    )\n+\n+\n def test_pipeline_transform():\n     # Test whether pipeline works with a transformer at the end.\n     # Also test pipeline.transform and pipeline.inverse_transform\n", "problem_statement": "Expose `verbose_feature_names_out` in `make_union`\n### Describe the workflow you want to enable\r\n\r\n```python\r\nfrom sklearn.pipeline import make_union\r\n\r\nfeature_union = make_union(..., verbose_feature_names_out=False)\r\n```\r\n\r\n### Describe your proposed solution\r\n\r\nAdd a keyword arg like in `make_column_transformer`\r\n\r\n### Describe alternatives you've considered, if relevant\r\n\r\nExplicitly defining with `FeatureUnion`\r\n\r\n### Additional context\r\n\r\nWould be a convenience \n", "hints_text": "Hi, I'd be interested in working on this feature request.\nThe option already exists in `FeatureUnion` so it would make sense to expose it in `make_union`.\r\n\r\n+1 for a PR. I could not spot any such open PR by doing a quick search.", "created_at": "2024-12-04T14:59:52Z"}
